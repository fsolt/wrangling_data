---
format:
  pdf:
    number-sections: true
    papersize: a4
    keep-tex: false
    include-in-header: 
      text: |
        \usepackage{typearea}
        \usepackage{endnotes}
        \let\footnote=\endnote
crossref:
  sec-prefix: OSM
  sec-labels: alpha A
    
author:
    - name: ""
      affiliations: ""
#  - name: Yue Hu
#    affiliations:
#      - ref: tsu
#    orcid: 0000-0002-2829-3971
#    email: yuehu@tsinghua.edu.cn
#    url: https://www.drhuyue.site
#  - name: Yuehong Cassandra Tai
#    affiliations:
#      - ref: psu
#    orcid: https://orcid.org/0000-0001-7303-7443
#    email: yhcasstai@psu.edu
#  - name: Frederick Solt
#    affiliations:
#      - ref: ia
#    orcid: 0000-0002-3154-6132
#    email: frederick-solt@uiowa.edu
#    url: https://www.fsolt.org
#affiliations:
#  - id: tsu
#    name: Department of Political Science, Tsinghua University, Beijing, China
#  - id: psu
#    name: Center for Social Data Analytics, Pennsylvania State University, University Park, USA
#  - id: ia
#    name: Department of Political Science, University of Iowa, Iowa City, USA
#thanks: "Corresponding author: Yuehong Cassandra Tai, [yhcasstai@psu.edu](mailto:yhcasstai@psu.edu). Current version: `r format(Sys.time(), '%B %d, %Y')`.  Replication materials and complete revision history may be found at [https://github.com/fsolt/wrangling_data](https://github.com/fsolt/wrangling_data). The authors contributed equally to this work.  Yue Hu appreciates the funding support from the National Natural Science Foundation of China (72374116) and Tsinghua University Initiative Scientific Research Program (2024THZWJC01)."
citeproc: false # to make multibib and wordcount work
filters:
#  - authors-block
  - multibib # separate bib for main and appendix
  - at: pre-render
    path: "_extensions/andrewheiss/wordcount/wordcount.lua"
validate-yaml: false # for multibib to work
bibliography: 
    main: "main_wrangling.bib"
citation_package: natbib
csl: "american-political-science-review.csl"
tables: true # enable longtable and booktabs
fontsize: 12pt
indent: true
geometry: margin=1in
linestretch: 1.5 # double spacing using linestretch 1.5
colorlinks: false
link-citations: true
execute:
  echo: false
  message: false
  warning: false
  dpi: 600
editor_options: 
  chunk_output_type: console
editor:
  render-on-save: true
title: |
  | A TAO for Data Wrangling:
  | A Practical Routine for Getting
  | Past the 'Janitor Work'
#subtitle: "Preliminary version. Do not circulate without permission."
#abstract: |
#    This article focuses on a preliminary step in any ex-post data harmonization project---wrangling the pre-harmonized data---and suggests a practical routine for helping researchers reduce human errors in this often-tedious work. 
#    The routine includes three steps: (1) *T*eam-based concept construct and data selection; (2) Data entry *a*utomation; and (3) "Second-order" *o*pening---a "Tao" of data wrangling. 
#    We illustrate the routine with the examples of pre-harmonizing procedures used to produce the Standardized World Income Inequality Database (SWIID), a widely used database that uses Gini indices from multiple sources to create comparable estimates, and the Dynamic Comparative Public Opinion (DCPO) project, which creates a workflow for harmonizing aggregate public opinion data.
#keywords: 
#  - Data generation process
#  - Machine assistance
#  - Manual coding
#  - Data cleaning
#  - Data management
---

```{r}
#| label: setup
#| include: false

# options(repos = c(CRAN = "https://cloud.r-project.org"))

# if (!require(cmdstanr)) {
#   install.packages("cmdstanr", repos = c(
#     "https://mc-stan.org/r-packages/",
#     getOption("repos")
#   ))
#   library(cmdstanr)
#   install_cmdstan() # C++ toolchain required; see https://mc-stan.org/cmdstanr/articles/cmdstanr.html
# }

if (!require(pacman)) {
  install.packages("pacman")
}
library(pacman)

# p_install(janitor, force = FALSE)
# p_install_gh(c("fsolt/DCPOtools"), force = FALSE)

# load all the packages you will use below
p_load(
  # analysis
  # cmdstanr,
  # plm,
  # osfr,

  # presentation
  gridExtra,
  modelsummary,
  dotwhisker,
  latex2exp,
  knitr,
  kableExtra,

  # data wrangling
  DCPOtools,
  janitor,
  countrycode,
  here,
  broom,
  tidyverse,
  glue,
  tinytable
)

theme_set(theme_minimal())
set.seed(313)

use("dplyr", c("filter", "select")) # require R 4.5.0

```

```{r}
#| label: preloadFuns

## Beck-Katz panel-corrected standard errors
vcovHC_se <- function(x) {
  plm::vcovHC(x, method = "arellano", cluster = "group") %>% # default setting
    diag() %>%
    sqrt()
}

## Tabulation -----------------------------------------------------------------------
na_types_dict <- list(
  "r" = NA_real_,
  "i" = rlang::na_int,
  "c" = NA_character_,
  "l" = rlang::na_lgl
)

### A function that converts a string to a vector of NA types.
### e.g. "rri" -> c(NA_real_, NA_real_, rlang::na_int)
parse_na_types <- function(s) {
  positions <- purrr::map(
    stringr::str_split(s, pattern = ""),
    match,
    table = names(na_types_dict)
  ) %>%
    unlist()

  na_types_dict[positions] %>%
    unlist() %>%
    unname()
}

### A function that, given named arguments, will make a one-row tibble, switching out NULLs for the appropriate NA type.
as_glance_tibble <- function(..., na_types) {
  cols <- list(...)

  if (length(cols) != stringr::str_length(na_types)) {
    stop(
      "The number of columns provided does not match the number of ",
      "column types provided."
    )
  }

  na_types_long <- parse_na_types(na_types)

  entries <- purrr::map2(
    cols,
    na_types_long,
    function(.x, .y) {
      if (length(.x) == 0) .y else .x
    }
  )

  tibble::as_tibble_row(entries)
}

tidy.pgmm <- function(x, conf.int = FALSE, conf.level = 0.95, ...) {
  result <- summary(x)$coefficients %>%
    tibble::as_tibble(rownames = "term") %>%
    dplyr::rename(
      estimate = Estimate,
      std.error = `Std. Error`,
      statistic = `z-value`,
      p.value = `Pr(>|z|)`
    )

  if (conf.int) {
    ci <- confint(x, level = conf.level) %>%
      as.data.frame() %>%
      rownames_to_column(var = "term") %>%
      dplyr::rename(
        conf.low = `2.5 %`,
        conf.high = `97.5 %`
      )
    result <- dplyr::left_join(result, ci, by = "term")
  }

  result
}

glance.plm <- function(x, ...) {
  s <- summary(x)
  as_glance_tibble(
    nobs = stats::nobs(x),
    n.country = pdim(x)$nT$n,
    na_types = "ii"
  )
}

glance.pgmm <- function(x, ...) {
  s <- summary(x)
  as_glance_tibble(
    nobs = stats::nobs(x),
    n.country = pdim(x)$nT$n,
    n.inst = dim(x$W[[1]])[2],
    na_types = "iii"
  )
}

## dotwhisker::small_multiple() hacks

body(small_multiple)[[19]] <- substitute(
  p <-
    ggplot(
      df,
      aes(
        y = estimate,
        ymin = conf.low,
        ymax = conf.high,
        x = as.factor(model),
        colour = submodel
      )
    ) +
    do.call(geom_pointrange, point_args) +
    ylab("") +
    xlab("") +
    facet_grid(
      term ~ .,
      scales = "free",
      labeller = label_parsed,
      # enable LaTeX facet labels
      switch = "y"
    ) + # put facet labels on left
    scale_y_continuous(position = "right") # put axis label on right
)
```

\pagebreak

# The Issue of Wrangling in Data Harmonization

Empowered by the spreading Internet and advancing computational power, researchers have entered an unprecedented age of data availability.
A growing volume of social science research aims to take the benefit to extend generality: they employ large quantities of data drawn from different sources.
However, significant challenges in ensuring the quality of harmonized datasets remain in handling fitness for use and monitoring raw data quality [@Slomczynsi2025].

The wrangling step determines the quality of data in the harmonization process, and the challenge is how to properly and transparently clean the increasing amount and diversity of data.
The conventional approach usually and notoriously involves a great deal of manual work on indicator identification, data merging, data scaling, and so on [see, e.g., @Lohr2014].
Manual wrangling undermines transparency and makes full reproducibility of the research pipeline more difficult to achieve [@Liu2019].
Worse, this tiresome task makes it easy to introduce errors in the data.
Finally, even meticulous documentation cannot eliminate the influence of human discretion embedded in manual processing.
Such discretion often leaves behind few traces, making it challenging for collaborators or reviewers to verify the wrangling process or diagnose sources of error.

In short, incomplete/inaccurate data entry, the absence of reproducibility, and untrackable human discretion in manual janitor work have collectively become the largest obstacle on the way to data harmonization, yet thus far this obstacle has gained little attention.
In this article, we provide a practical routine (a "TAO") that takes advantage of automatic programming and teamwork to reduce data-entry errors and improve the reproducibility and transparency of the wrangling process for researchers and reviewers.
This TAO covers the three phases of data wrangling: data selection and collection, data entry, and data-generation-process(DGP) opening.
We illustrate how researchers can use this routine on statistics and survey data with examples of two ongoing harmonization efforts, the Standardized World Income Inequality Database (SWIID) and the Dynamic Comparative Public Opinion (DCPO) project.


# A 3-Step "TAO" for Data Wrangling

Our routine aims to helping researchers reach three goals for scientific study: 

1. To reduce the manual entry errors to improve the accuracy of the harmonized data and analytic data; 
1. To incorporate as much available data as possible to provide a solid base for comparable data and increase generality of the inferences;^[
  Here, “available data” refers to existing data sources that contain information relevant to the variable of interest.
  Although some comparisons can proceed without complete data coverage, analyses based on incomplete information risk producing misleading conclusions [see, e.g., @SoltEtAl2016; @SoltEtAl2017].
  Therefore, the sufficiency of data is often closely tied to the (external) validity of comparisons.
]
and
1. To improve the reproducibility of data wrangling process for the sake of transparency.

The routine decomposes a data-wrangling process into three steps: 

1. **T**eam-based concept construct and data selection;
1. Data entry **a**utomation; and 
1. "Second-order" **o**pening.

We use two data harmonization projects, SWIID and DCPO, to illustrate this routine.
SWIID is a long-running project that seeks to provide harmonized income inequality statistics for the broadest possible coverage of countries and years [@Solt2020].
As of its most recent update at the time of this writing, its source data consists of more than 27,000 observations of the Gini coefficient of income distribution in nearly 200 countries over as many as 65 years, collected from over 400 separate sources including international organizations, national statistics bureaus, and academic studies.

DCPO is both a method and a database.
Scholarship on comparative public opinion only rarely benefits from relevant items asked annually by the same survey in many countries [see, e.g., @HagemannEtAl2017].
To address the lack of cross-national and longitudinal data on many topics, a number of works have presented latent variable models that harmonize survey items that intend to capture the identical concepts but with different question wordings or option scales [see e.g., @CaugheyEtAl2019; @Claassen2019].
Advancing this line of work, DCPO not only provides latent variable measurements but also automated and reproducible data collection [@Solt2020], which has been applied in a complete pipeline for a variety of topics such as gender egalitarianism [@Woo2023], political interest [@Hu2025a], and support for gay rights [@Woo2025], among other aspects of public opinion and open it freely for global researchers (see the data available at <https://dcpo.org/>).


## Step 1: Team-Based Construct Building and Data Selection

Large-scale data selection and cleaning is often viewed as tiresome, as something to be delegated to research assistants, to someone---indeed anyone---else.
Performing these tasks manually makes it easy to make mistakes and errors.
@HaegemansEtAl2019 [p. 1] lists examples of misrouted financial transactions and airline flights.
In a more systematic examination, @BarchardPace2011 found that RAs assigned in an experiment to carefully enter data manually and instructed to prioritize accuracy over speed still had error rates approaching 1% in just a single roughly half-hour session.
The consequences of such errors are pernicious, undermining our results and more broadly our confidence in the scientific enterprise.

Our antidote for this issue is a combination of teamwork and automation. 
We will focus first on teamwork and discuss the latter in @sec-automate.
The goals here are to have consistent understanding on the conceptualized construct, to select valid data for later measurement and/or analyses, and to reduce biases caused by inconsistent human judgment.
A teamwork process to these ends requires using a deliberative set and a dual-entry process.

A deliberative set requires the members in a research team---whether several coauthors or a primary author with one or more RAs---to have a clear and coherent understanding of the research questions and associated data goals.
These understandings will help the team members identify the right data to collect and discover extra useful data sources that are not in the initial plan.

In the early years of the SWIID program, for example, RAs were told that the goal of the project is to generate comparable statistics of country-level economic inequality.
They were provided a list of sources to start with, mainly from national statistic bureaus, but also told that updated statistics for some countries may come from academic papers, published documents, and other sources, and encouraged to add each of these new sources by recording a valid link.

In the DCPO project, clearly defining and agreeing upon the latent construct among team members is a critical first step for ensuring theoretical comparability across countries and over time [@Koc2025]. 
This process begins with a shared conceptual foundation established through literature review and the corresponding pre-defined potential dimensions of latent opinion.
Each team member is then assigned survey datasets from specific geographic regions and tasked with identifying potentially relevant items and potential dimensions based on both general theoretical guidance and region-specific knowledge. 
This structure ensures that the construct is informed by both global theory and local context.
(For a more detailed checklist, see @sec-checklist.)

Before data selection begins, team members undergo hands-on training on how the method works and what types of data and metadata they need to collect, such as data format and weighting types, that are essential for enabling the automated data preparation process. 

Following the initial round of item selection and collection, the dual-entry section begins. 
In this stage, each team member reviews and re-codes the survey data originally handled by another member.
The independently coded versions are then compared to detect discrepancies, which may arise from misinterpretations of the construct, ambiguous item wording, or data-entry errors.

Disputed cases are flagged for group discussion.
Some mismatches may indicate items that may not be conceptually equivalent across cultures or regions, and others suggest multidimensionality that requires theoretical disaggregation.
For the latter, we either categorize such items into pre-defined dimensions and/or revise the codebook accordingly to add new dimensions---an iterative process aimed at improving construct validity, intercoder reliability, and reducing oversimplification of the target variable [@Slomczynsi2025].

Therefore, multiple lab meetings are held throughout the data selection phase to share insights from each member's coding work and ensure conceptual alignment across the team.
The process concludes with a final cross-check of the selected items by all members.

In addition to reducing manual biases, teamwork also helps expand the data pool.
Both SWIID and DCPO projects enrolled team members from outside the United States.
These members draw on their linguistic and cultural expertise to detect extra sources in non-English languages and improve the precision of the data selection.
To some extent, data from different sources help correct the biases caused by the survey designers' cultural backgrounds. 


## Step 2: Data Entry Automation {#sec-automate}

Putting the data into a format ready to merge is arguably the step most prone to manual errors and controversies.
The best solution is to automate the entry process, taking advantage of scripting and any application programming interfaces (APIs) provided by the data source.

In the DCPO case, data entry is fully automated through the R-based software, `DCPOtools` [@SoltEtAl2018a]. 
This software processes raw survey files directly, ensuring reproducible data entry. 
It converts various file formats to R-readable objects, extracts variables of interest, reorders response values, applies survey weights, and aggregates weighted respondents by country and year based on actual fieldwork dates.^[
  See more details how `DCPOtools` helps process the public surveys into the consistent format for harmonization in the upcoming vignette associated with the package and the software repo (https://github.com/fsolt/DCPOtools).
]

To address theoretical comparability concerns, DCPO employs conservative filtering, removing items appearing in fewer than five country-years in countries surveyed at least three times, minimizing the risk of sacrificing comparability for coverage [@Koc2025].
`DCPOtools` standardizes country names using @Arel-BundockEtAl2018's `countrycode` and ensures years reflect actual fieldwork dates, creating aggregated respondent data for the latent variable model.

```{r}
#| label: swiidData
#| include: false

api <- c(
  "LISSY",
  "CEPALStat",
  "OECD",
  "Eurostat",
  "Beegle et al. 2016",
  "Statistics Canada",
  "Statistics Denmark",
  "Statistics Finland",
  "CSO Ireland",
  "Statistics Norway",
  "Statistics Sweden",
  "World Bank Poverty & Inequality Platform",
  "Statbank Greenland"
)

sheet <- c(
  "SEDLAC",
  "Transmonee 2012",
  "Personal communication, K. Beegle, 2016-08-01",
  "World Bank Povcalnet",
  "Australian Bureau of Statistics",
  "Instituto Naciónal de Estadística de Bolivia",
  "Instituto de Pesquisa Económica Aplicada",
  "Departamento Administrativo Nacional de Estadística Colombia",
  "Instituto Naciónal de Estadística y Censos Costa Rica",
  "Central Agency for Public Mobilization and Statistics Egypt",
  "Statistics Estonia",
  "Statistics Georgia",
  "Statistics Hong Kong 2017",
  "Statistics Indonesia",
  "Istat",
  "Statistical Institute of Jamaica",
  "Kazakhstan Committee on Statistics",
  "Statistics Korea",
  "National Statistical Committee of Kyrgyzstan",
  "National Bureau of Statistics of Moldova",
  "Statistical Office of Montenegro",
  "Statistics New Zealand 1999",
  "Philippines Statistical Agency",
  "Russian Federal State Statistics Service",
  "Singapore Department of Statistics",
  "Slovenia Statistics Office",
  "Slovenia Statistics Office 2005",
  "Instituto Nacional de Estadística Spain",
  "Switzerland Federal Statistics Office",
  "Taiwan Directorate General of Budget, Accounting, and Statistics",
  "Turkish Statistical Institute",
  "UK Office for National Statistics",
  "Institute for Fiscal Studies",
  "U.S. Congressional Budget Office",
  "U.S. Census Bureau",
  "Instituto Nacional de Estadística Venezuela",
  "Milanovic 2016",
  "Milanovic 2016; Brandolini 1998",
  "Ackah, Bussolo, De Hoyos, and Medvedev 2008",
  "NESDC Thailand",
  "U.S. Census Bureau 1998",
  "Dirección General de Estadística, Encuestas y Censos"
)
scrape <- c(
  "National Statistical Service of Armenia",
  "Belarus National Committee of Statistics",
  "Statistics Hong Kong 2012",
  "Statistics Hong Kong 2007",
  "Dirección General de Estadística, Encuestas y Censos 2016",
  "Economy Planning Unit of Malaysia",
  "Perry 2018",
  "Dirección General de Estadística, Encuestas y Censos 2017",
  "Statistics Sri Lanka 2015",
  "NESDB Thailand",
  "Instituto Nacional de Estadística Uruguay",
  "General Statistics Office of Vietnam 2013",
  "General Statistics Office of Vietnam", # <- scrape from pdf
  # webscrape ->
  "Institut National de la Statistique et des Études Économiques France",
  "Statistical Center of Iran",
  "National Statistical Office of Thailand"
)

length(api) <- length(sheet)
length(scrape) <- length(sheet)

mode <- tibble(api, sheet, scrape) %>%
  gather(key = mode, value = source1) %>%
  filter(!is.na(source1))

## Merely to deal with the Internet under GFW
## Enable the following when making the replication file
# swiid_source <- read_csv(
#   "https://raw.githubusercontent.com/fsolt/swiid/master/data/swiid_source.csv",
#   col_types = "cdddcclcccc"
# ) %>%
#   left_join(mode, by = "source1") %>%
#   mutate(mode = if_else(is.na(mode), "hand", mode))

swiid_source <- rio::import(here("data", "swiid_source.csv")) %>%
  left_join(mode, by = "source1") %>%
  mutate(mode = if_else(is.na(mode), "hand", mode))

wordify_numeral <- function(x)
  setNames(
    c(
      "one",
      "two",
      "three",
      "four",
      "five",
      "six",
      "seven",
      "eight",
      "nine",
      "ten",
      "eleven",
      "twelve",
      "thirteen",
      "fourteen",
      "fifteen",
      "sixteen",
      " seventeen",
      "eighteen",
      "nineteen"
    ),
    1:19
  )[x]

api_percent <- swiid_source %>%
  count(mode == "api") %>%
  mutate(p = round(n / nrow(swiid_source) * 100)) %>%
  filter(`mode == "api"` == TRUE) %>%
  pull(p)

automated_percent <- swiid_source %>%
  count(mode == "hand") %>%
  mutate(p = round(n / nrow(swiid_source) * 100)) %>%
  filter(`mode == "hand"` == FALSE) %>%
  pull(p)

```

```{r}
#| label: fig-dataMethod
#| echo: false
#| fig-cap: "Income Inequality Observations by Method of Collection"
#| fig-height: 2
#| fig-width: 4

swiid_source %>%
  count(mode) %>%
  mutate(
    method = fct_recode(
      factor(mode),
      "API" = "api",
      "Spreadsheet\nDownload" = "sheet",
      "Web/PDF\nScrape" = "scrape",
      "Hand\nEntered" = "hand"
    ) %>%
      fct_relevel(
        "Hand\nEntered",
        "Web/PDF\nScrape",
        "Spreadsheet\nDownload",
        "API"
      )
  ) %>%
  ggplot(aes(x = n, y = method, fill = method)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  theme(axis.title.y = element_blank()) +
  xlab("Observations") +
  scale_fill_manual(
    "Legend",
    values = c(
      "Hand\nEntered" = "gray75",
      "Web/PDF\nScrape" = "gray10",
      "Spreadsheet\nDownload" = "gray10",
      "API" = "gray10"
    )
  ) +
  theme(legend.position = "none")

# Hand-Entered and Machine-Collected
```

While coding datasets and items into structured spreadsheets facilitates automation, an even better approach starts the automation from the data selection step via scripting and APIs.
As shown in @fig-dataMethod, the current version of SWIID collects `r api_percent`% of the observations through APIs.
In @sec-packages, we provide an illustrative list of R packages that can assist with tasks such as collecting data via APIs and cleaning and transforming data.
The list is long but far from complete.
Readers are welcome to modify the arguments of the codes (such as the keywords used to select packages) in this article's replication file that we use to create the list, they will discover many times more packages that already exist to help collect and wrangle data.

Returning to the case of the SWIID: when no API is available, the automation script downloads and reads any available spreadsheets.
In the absence of a spreadsheet, the process of scraping the data either directly from the web or, preferably, from a pdf file [see @Sepulveda2024] is automated.
Together the collection of `r automated_percent`% of the source data is scripted.
This means not only that errors associated with manual data entry for the majority of observations are eliminated but also that ongoing updates and revisions can be easily integrated when they become avaliable and the data collecion program is reran.

Even for data sources that have to be entered in hand, such as those from academic articles or books, there is still opportunity for partial automation.
For the remaining `r 100 - automated_percent`% of the SWIID observations, for instance, many were collected using Sepulveda's `tabulapdf` R package to avoid data-entry errors.
Optical Character Recognition (OCR) can be used to extend this method to even hard-copy data sources.
Finally, for data that one has to enter manually, the teamwork is crucial.
Mirroring the approach for data selection described above, each hand-entered observation was independently entered twice into two separate spreadsheets.
The dual-entry process allows for automated cross-checks of the newly entered data that increase the chances that errors are identified and corrected [see @BarchardPace2011].


## Step 3: "Second-Order" Opening

Since the replication crisis, replication files for analytical results in academic articles has become a standard requirement for top-tier journals in political science [@ChangLi2015; @OpenScienceCollaboration2015]. 
Nevertheless, issue of researcher degrees of freedom indicates that current degree of openness is frequently still inadequate [see @HuTaiSolt2025].
Especially in relation with data harmonization, we encourage researchers to conduct what we will call a "second-order" opening of their research process. 
This involves not only opening one's analytical steps (the first-order opening common today) but also one's DGP (the second-order opening), including data collection, cleaning, and wrangling.

Empirical evidence has indicated the severe consequences of neglecting second-order opening.
Recent research has found that the variation in the estimated effects caused by researchers may outweigh the population's variation [@Holzmeister2024].
Within this researcher-choice variation, a substantial portion comes from the data-wrangling process [@Huntington-Klein2025, 33].
In a "many-analyst" analysis, @Huntington-Klein2025 requested 146 research teams to complete the same research task, in which group A decided how to accomplish the task all by themselves, group B was given a specified research design, and group C was given the same research design and a pre-cleaned data set.
The study found that actually it is group B that generated the highest outcome variation---even higher than group A.
The teams who were given the pre-cleaned data set generated the lowest outcome variation.
These findings indicate that the research replicability cannot be secured by first-order opening without second-order opening.

If researchers apply our suggestions of team-based construct building, systematic data selection, and automated data entry, the second-order opening will be both feasible and efficient.
Along with a clearly conceptualized theoretical framework, researchers can simply share their programming scripts for data downloading, formatting, and wrangling, and thereby ensure that the full pipeline is documented and reproducible.

With developed scientific and technical publishing system, such as Quarto or R Markdown, version control platforms like Github, and open collaboration platforms including the Open Science Framework, researchers can integrate the entire workflow—--from raw data collection to final analysis--—within a single, publicly trackable archive.
We reached at this step for all the DCPO projects so far. 
Readers can trace a research project from the start in a Github repo and every wave of data update in the corresponding OSF project [see, for example, @TaiEtAl2024].


# Discussion

![The TAO of Data Wrangling Before Data Harmonization. Source: Self generated.](images/data_diagram.png){#fig-diagram fig-align="center" height=600}

@fig-diagram presents the whole process of the 3-step routine of data wrangling for later harmonization phase.
Implementing these practices requires effort, just as in many open-science endeavors [see @EngzellRohrer2021]. 
Though labor-intensive, the double-entry method reduces error rates thirty-fold [@BarchardPace2011, p. 1837], which easily justifies the additional investment. 
Teamwork fosters conceptual alignment and construct refinement through collaborative discussion while also distributing tasks to reduce fatigue-related errors.

Social scientists now benefit from standardized harmonization workflows [@Slomczynsi2025] and automated data processing [@Kritzinger2025]. 
Researchers can reuse high-quality harmonized datasets, enhancing efficiency and comparability.
Open-source software packages like those used by the SWIID and `DCPOtools` have already automated many data preparation tasks. 
With large language models emerging, intelligent agents may soon handle parts of these routines, potentially advancing automation to new levels [@Kritzinger2025].

A final point we would like to emphasize is that, in our three-step routine, researchers remain central to data harmonization. 
As illustrated in the SWIID and DCPO examples, researchers are responsible for all critical decisions from clarifying research questions and building theoretical constructs to conducting version control and developing replication materials.
Early and critical steps, such as construct development and codebook refinement, must be conducted iteratively to achieve high intercoder reliability. 
Even with automated data entry, human validation remains essential for verifying variable formats and value ranges. 
Computing environments should be documented to minimize system-related discrepancies [@Liu2019].

For ex-post harmonization projects, careful attention to pre-harmonization stages substantially contributes to overall data quality. 
While some error is inevitable, with responsible researcher oversight, data-entry errors can be minimized while transparency, openness, and research credibility continue to grow.


\newpage

# References {.unnumbered}

::: {#refs-main}
:::

\clearpage
\newpage

\hypertarget{appendix-appendix}{%
\appendix}

# Online Supplementary Materials {.unnumbered}

```{r}
#| label: setup-app
#| include: false

p_load("pkgsearch")
#options(tinytable_theme_placement_latex_float = "H")
```

\setcounter{page}{1}
\renewcommand{\thepage}{A\arabic{page}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}

# Checklist for Deliberation Process {#sec-checklist}

@tbl-checklist is a checklist with notes or rationales for key decisions made during the deliberation process.
The focus on each step may vary depending on the research purpose.
For example, in public opinion harmonization projects like DCPO, more time is typically devoted to conceptualization and construct development compared to administrative data projects such as SWIID.
However, this general checklist can serve as a useful guide across a range of harmonization efforts.

```{r}
#| label: tbl-checklist
#| tbl-cap: Checklist with Decision Rationale

deliberation_data <- data.frame(
  Step = c(
    "1.Clarify Conceptual Construct",
    "1.Clarify Conceptual Construct",
    "1.Clarify Conceptual Construct",
    
    "2.Document Research Goals",
    "2.Document Research Goals",
    "2.Document Research Goals",

    "3.Assign Data Collection",
    "3.Assign Data Collection",
    "3.Assign Data Collection",
    
    "4.Dual-Entry and Cross-Check",
    "4.Dual-Entry and Cross-Check",
    "4.Dual-Entry and Cross-Check",
    
    "5.Deliberation on Discrepancies",
    "5.Deliberation on Discrepancies",
    "5.Deliberation on Discrepancies",
    
    "6.Log Data and Decision",
    "6.Log Data and Decision"
  ),
  Checklist = c(
    "Literature review and shared across the team.",
    "Confirm shared understanding of theoretical construct.",
    "Relevant theoretical dimensions are discussed and documented.",

    "Instructions on key variable formats and downstream analytical needs.",
    "Review initial codebook.",
    "Data input training.",

    "Assign datasets to team members by geography or source.",
    "Each team member maintains a seperate sheet for raw data collection and a log of decisions.",
    "",

    "Conduct dual entry by second team member.",
    "Discrepancies flagged and logged for group discussion.",
    "",

    "Team discussion on discrepancies.",
    "Items with unclear mapping to conceptual dimensions are categorized or excluded.",
    "Update codebook/documentation.",

    "Finalize data by cross-check of all members",
    "Log all decisions and changes in version-controlled repository (e.g., OSF, GitHub)."
  ),

  Notes = c(
    "Notes from team discussion",
    "",
    "",
    "Update when necessary",
    "",
    "",
    "Document ambiguous items",
    "",
    "",
    "Record discrepancies found",
    "",
    "",
    "Provide examples of key disputes and how they were resolved",
    "",
    "",
    "Mention major updates.",
    ""
  )
)


tt(
  deliberation_data,
  width = c(0.3, 0.5, 0.2)
) |>
  style_tt(fontsize = 0.7) |>
  style_tt(i = c(1, 4, 7, 10, 13), j = c(1, 3), rowspan = 3, fontsize = 0.7) |>
  style_tt(i = c(8, 11), j = c(2), rowspan = 2, fontsize = 0.7) |>
  style_tt(i = c(16), j = c(1, 3),rowspan = 2, fontsize = 0.7) |>
  theme_tt("grid")

```



# R packages for data wrangling {#sec-packages}

Here are exemplary R packages that researchers can use to collect, clean, and transform data.
The following tables were generated by the `pkgsearch::pkg_search()` function with the keywords relating to data downloading, wrangling, and transforming.
The packages are ranked based on the 'score' metric that reflects both textual relevances with the keyword and package popularity in the last month.
Only the top twenty packages and only the maintainers' names are shown.
We encourage readers to use the codes in this paper's replication file to explore more useful packages.
We also recommend readers to refer to the ["CRAN Task View: Reproducible Research"](https://cran.r-project.org/web/views/ReproducibleResearch.html) page for more useful tools to achieve the first-order and second-order opening.

```{r}
#| label: packages

kw_download <- c("api download", "collect", "gather")
kw_clean <- c("preprocess data", "data clean", "parse", "sanitize", "scrub",  "correct", "rectify", "standardize")
kw_transform <- c("convert data", "reformat", "transform", "aggregate", "rescale", "reshape", "recode", "modify", "restructure")

kw_short <- c(kw_download[1], kw_clean[1], kw_transform[1])

ls_packages <- map(kw_short, \(keyword){
  pkg_search(keyword, format = "long", size = 50) |> 
    select(package, title, maintainer = maintainer_name) 
}) |> 
  set_names(c("download", "clean", "transform"))
```

```{r}
#| label: software-download

tt(ls_packages$download, 
  width = c(0.2, 0.5, 0.3),
  caption = "Example packages for downloading data with API\\label{tbl-download}") |> 
    theme_latex(tabularray_inner = "stretch=0",
      multipage = TRUE) |> 
  style_tt(fontsize = 0.7)

```

```{r}
#| label: software-clean

tt(ls_packages$clean, 
  width = c(0.2, 0.5, 0.3),
  caption = "Example packages for cleaning data with API\\label{tbl-clean}") |> 
    theme_latex(tabularray_inner = "stretch=0",
      multipage = TRUE) |> 
  style_tt(fontsize = 0.7)

```

```{r}
#| label: software-transform

tt(ls_packages$transform, 
  width = c(0.2, 0.5, 0.3),
  caption = "Example packages for transforming data with API\\label{tbl-transform}") |> 
    theme_latex(tabularray_inner = "stretch=0",
      multipage = TRUE) |> 
  style_tt(fontsize = 0.7)

```