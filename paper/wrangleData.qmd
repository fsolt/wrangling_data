---
format:
  pdf:
    number-sections: true
    papersize: a4
    keep-tex: false
    include-in-header: 
      text: |
        \usepackage{typearea}
        \usepackage{endnotes}
        \let\footnote=\endnote

crossref:
  sec-prefix: OSM
  sec-labels: alpha A
    
author:
  - name: Yue Hu
    affiliations:
      - ref: tsu
    orcid: 0000-0002-2829-3971
    email: yuehu@tsinghua.edu.cn
    url: https://www.drhuyue.site
  - name: Yuehong Cassandra Tai
    affiliations:
      - ref: psu
    orcid: https://orcid.org/0000-0001-7303-7443
    email: yhcasstai@psu.edu
  - name: Frederick Solt
    affiliations:
      - ref: ia
    orcid: 0000-0002-3154-6132
    email: frederick-solt@uiowa.edu
    url: https://www.fsolt.org
affiliations:
  - id: tsu
    name: Department of Political Science, Tsinghua University, Beijing, China
  - id: psu
    name: Center for Social Data Analytics, Pennsylvania State University, University Park, USA
  - id: ia
    name: Department of Political Science, University of Iowa, Iowa City, USA
thanks: "Corresponding author: [yuehong-tai@uiowa.edu](mailto:yuehong-tai@uiowa.edu). Current version: `r format(Sys.time(), '%B %d, %Y')`.  Replication materials and complete revision history may be found at [https://github.com/fsolt/wrangling_data](https://github.com/fsolt/wrangling_data). The authors contributed equally to this work.  Yue Hu appreciates the funding support from the National Natural Science Foundation of China (72374116) and Tsinghua University Initiative Scientific Research Program (2024THZWJC01)."
citeproc: false # to make multibib and wordcount work
filters:
  - authors-block
  - multibib # separate bib for main and appendix
  - at: pre-render
    path: "_extensions/andrewheiss/wordcount/wordcount.lua"
validate-yaml: false # for multibib to work
bibliography: 
    main: "main_wrangling.bib"
    appendix: "app_wrangling.bib"
citation_package: natbib
csl: "american-political-science-review.csl"
tables: true # enable longtable and booktabs
fontsize: 12pt
indent: true
geometry: margin=1in
linestretch: 1.5 # double spacing using linestretch 1.5
colorlinks: false
link-citations: true
execute:
  echo: false
  message: false
  warning: false
  dpi: 600
editor_options: 
  chunk_output_type: console
editor:
  render-on-save: true
title: |
  | On 'Janitor Work' in Political Science:
  | Best Practices for Wrangling Data
  | Before Harmonization
# subtitle: "Preliminary version. Do not circulate without permission."
abstract: "This article focuses on a preliminary step in any ex-post data harmonization project---wrangling the pre-harmonized data---and suggests best practices for helping scholars avoid errors in this often-tedious work. To provide illustrations of these best practices, the article uses the examples of pre-harmonizing procedures used to produce the Standardized World Income Inequality Database (SWIID), a widely used database that uses Gini indices from multiple sources to create comparable estimates, and the Dynamic Comparative Public Opinion (DCPO) project, which creates a workflow for harmonizing aggregate public opinion data."

keywords: 
  - Data generation process
  - Machine assistance
  - Manual coding
  - Data cleaning
  - Data management

---

```{r}
#| label: setup
#| include: false

options(repos = c(CRAN = "https://cloud.r-project.org"))

 if (!require(cmdstanr)) {
  install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/",
                                         getOption("repos")))
  library(cmdstanr)
  install_cmdstan() # C++ toolchain required; see https://mc-stan.org/cmdstanr/articles/cmdstanr.html
}

if (!require(pacman))
    install.packages("pacman")
library(pacman)
  
p_install(janitor, force = FALSE)
p_install_gh(c("fsolt/DCPOtools"), force = FALSE)

# load all the packages you will use below
p_load(
    # analysis
    cmdstanr,
    plm,
    osfr,
    
    # presentation
    gridExtra,
    modelsummary,
    dotwhisker,
    latex2exp,
    
    # data wrangling
    DCPOtools,
    janitor,
    countrycode,
    here,
    broom,
    tidyverse,
    glue)
 
# Functions preload
theme_set(theme_minimal())
set.seed(313)
 
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("filter", "dplyr")
 
# Function preload ------------------------------------------------------------------

## Beck-Katz panel-corrected standard errors
vcovHC_se <-  function(x) {
    plm::vcovHC(x, method="arellano", cluster="group") %>%  #default setting
        diag() %>% 
        sqrt()
}

## Tabulation -----------------------------------------------------------------------
na_types_dict <- list("r" = NA_real_,
                      "i" = rlang::na_int,
                      "c" = NA_character_,
                      "l" = rlang::na_lgl)

### A function that converts a string to a vector of NA types.
### e.g. "rri" -> c(NA_real_, NA_real_, rlang::na_int)
parse_na_types <- function(s) {
    
    positions <- purrr::map(
        stringr::str_split(s, pattern = ""), 
        match,
        table = names(na_types_dict)
    ) %>%
        unlist()
    
    na_types_dict[positions] %>%
        unlist() %>%
        unname()
}

### A function that, given named arguments, will make a one-row tibble, switching out NULLs for the appropriate NA type.
as_glance_tibble <- function(..., na_types) {
    
    cols <- list(...)
    
    if (length(cols) != stringr::str_length(na_types)) {
        stop(
            "The number of columns provided does not match the number of ",
            "column types provided."
        )
    }
    
    na_types_long <- parse_na_types(na_types)
    
    entries <- purrr::map2(cols, 
                           na_types_long, 
                           function(.x, .y) {if (length(.x) == 0) .y else .x})
    
    tibble::as_tibble_row(entries)
    
}

tidy.pgmm <- function(x,
                      conf.int = FALSE,
                      conf.level = 0.95,
                      ...) {
    result <- summary(x)$coefficients %>%
        tibble::as_tibble(rownames = "term") %>%
        dplyr::rename(
            estimate = Estimate,
            std.error = `Std. Error`,
            statistic = `z-value`,
            p.value = `Pr(>|z|)`
        )
    
    if (conf.int) {
        ci <- confint(x, level = conf.level) %>% 
            as.data.frame() %>% 
            rownames_to_column(var = "term") %>% 
            dplyr::rename(
                conf.low = `2.5 %`,
                conf.high = `97.5 %`
            )
        result <- dplyr::left_join(result, ci, by = "term")
    }
    
    result
}

glance.plm <- function(x, ...) {
    s <- summary(x)
    as_glance_tibble(
        nobs = stats::nobs(x),
        n.country = pdim(x)$nT$n,
        na_types = "ii"
    )
}

glance.pgmm <- function(x, ...) {
    s <- summary(x)
    as_glance_tibble(nobs = stats::nobs(x),
        n.country = pdim(x)$nT$n,
        n.inst = dim(x$W[[1]])[2],
        na_types = "iii"
    )
}

## dotwhisker::small_multiple() hacks

body(small_multiple)[[19]] <- substitute(
    p <-
        ggplot(
            df,
            aes(
                y = estimate,
                ymin = conf.low,
                ymax = conf.high,
                x = as.factor(model),
                colour = submodel
            )
        ) +
        do.call(geom_pointrange, point_args) +
        ylab("") + xlab("") +
        facet_grid(
            term ~ .,
            scales = "free",
            labeller = label_parsed,
            # enable LaTeX facet labels
            switch = "y"
        ) +             # put facet labels on left
        scale_y_continuous(position = "right") # put axis label on right
)

```

\pagebreak
# The Problem with 'Janitor Work'
Most data harmonization projects---and a growing volume of other political science research---can be characterized as data science: they employ large quantities of data, often drawn from a large number of different sources.
For such projects, data wrangling, the task of getting these data into the format required to perform harmonization or analysis, is notoriously the bulk of the work [see, e.g., @Lohr2014].
Such 'janitor work' is often viewed as tiresome, as something to be delegated to research assistants, to someone---indeed anyone---else [see @Torres2017].
Data wrangling is, however, critically important to scientific inquiry, and errors that arise during this process can undermine our data-harmonization goals.

One kind of data-wrangling error presents a particularly insidious problem: errors that occur during manual data entry.
Faced with the task of getting data into the correct format before harmonizing, even some very sophisticated researchers will conclude that the most straightforward means to that end is to simply copy the needed data into a spreadsheet by hand.
This technique may be straightforward, but it is very much prone to error.
@Barchard2011 found that 'research assistants' assigned in an experiment to carefully enter data manually, even those instructed to double-check their entries against the original, had error rates approaching 1% in just a single roughly half-hour session.
Rates likely go up as the tedious task goes on.
Although the pernicious consequences of data-entry errors are easily grasped in everyday contexts---@Haegemans2019 [, 1] collects examples of misrouted financial transactions and airline flights---they have thus far gained little attention in political science, even among those working to harmonize large quantities of data.

We suggest three best practices for reducing the rate of data-entry errors.
First, _automate data entry_ to the greatest extent possible.
Second, _use the double-entry method_: when manual data entry cannot be avoided, each entry should be made twice, either by separate researchers or sequentially.
Third, _embrace teamwork_ for any project involving entering data by hand, splitting the task up among team members will reduce the risk of errors going undetected.
We demonstrate the application of these practices within two ongoing harmonization efforts, the Standardized World Income Inequality Database (SWIID) and the Dynamic Comparative Public Opinion (DCPO) project.


# Wrangling Income Inequality Data for Harmonization

```{r data-setup, include=FALSE}
api <- c("LISSY", "CEPALStat", "OECD", "Eurostat", "Beegle et al. 2016", "Statistics Canada", "Statistics Denmark", "Statistics Finland", "CSO Ireland", "Statistics Norway", "Statistics Sweden", "World Bank Poverty & Inequality Platform", "Statbank Greenland")
sheet <- c("SEDLAC", "Transmonee 2012", "Personal communication, K. Beegle, 2016-08-01", "World Bank Povcalnet", "Australian Bureau of Statistics", "Instituto Naciónal de Estadística de Bolivia", "Instituto de Pesquisa Económica Aplicada", "Departamento Administrativo Nacional de Estadística Colombia", "Instituto Naciónal de Estadística y Censos Costa Rica", "Central Agency for Public Mobilization and Statistics Egypt", "Statistics Estonia", "Statistics Georgia", "Statistics Hong Kong 2017", "Statistics Indonesia", "Istat", "Statistical Institute of Jamaica", "Kazakhstan Committee on Statistics", "Statistics Korea", "National Statistical Committee of Kyrgyzstan", "National Bureau of Statistics of Moldova", "Statistical Office of Montenegro", "Statistics New Zealand 1999", "Philippines Statistical Agency", "Russian Federal State Statistics Service", "Singapore Department of Statistics", "Slovenia Statistics Office", "Slovenia Statistics Office 2005", "Instituto Nacional de Estadística Spain", "Switzerland Federal Statistics Office", "Taiwan Directorate General of Budget, Accounting, and Statistics", "Turkish Statistical Institute", "UK Office for National Statistics", "Institute for Fiscal Studies", "U.S. Congressional Budget Office", "U.S. Census Bureau", "Instituto Nacional de Estadística Venezuela", "Milanovic 2016", "Milanovic 2016; Brandolini 1998", "Ackah, Bussolo, De Hoyos, and Medvedev 2008", "NESDC Thailand", "U.S. Census Bureau 1998", "Dirección General de Estadística, Encuestas y Censos")
scrape <- c("National Statistical Service of Armenia", "Belarus National Committee of Statistics", "Statistics Hong Kong 2012", "Statistics Hong Kong 2007", "Dirección General de Estadística, Encuestas y Censos 2016", "Economy Planning Unit of Malaysia", "Perry 2018", "Dirección General de Estadística, Encuestas y Censos 2017", "Statistics Sri Lanka 2015", "NESDB Thailand", "Instituto Nacional de Estadística Uruguay", "General Statistics Office of Vietnam 2013", "General Statistics Office of Vietnam", # <- scrape from pdf
            # webscrape ->
            "Institut National de la Statistique et des Études Économiques France", "Statistical Center of Iran", "National Statistical Office of Thailand")

length(api) <- length(sheet)
length(scrape) <- length(sheet)

mode <- tibble(api, sheet, scrape) %>% 
    gather(key = mode, value = source1) %>% 
    filter(!is.na(source1))

swiid_source <- read_csv("https://raw.githubusercontent.com/fsolt/swiid/master/data/swiid_source.csv", 
                         col_types = "cdddcclcccc") %>% 
    left_join(mode, by = "source1") %>% 
    mutate(mode = if_else(is.na(mode), "hand", mode))

wordify_numeral <- function(x) setNames(c("one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen", "sixteen", " seventeen", "eighteen", "nineteen"), 1:19)[x]

api_percent <- swiid_source %>% count(mode == "api") %>% mutate(p = round(n/nrow(swiid_source) * 100)) %>% filter(`mode == "api"` == TRUE) %>% pull(p)

automated_percent <- swiid_source %>% count(mode == "hand") %>% mutate(p = round(n/nrow(swiid_source) * 100)) %>% filter(`mode == "hand"` == FALSE) %>% pull(p)

```

The Standardized World Income Inequality Database (SWIID) is a long-running project that seeks to provide harmonized income inequality statistics for the broadest possible coverage of countries and years [@Solt2009; @Solt2015; @Solt2016; @Solt2020].
As of its most recent update at the time of this writing, its source data consists of some 27,000 observations of the Gini coefficient of income distribution in nearly 200 countries over as many as 65 years, collected from over 400 separate sources including international organizations, national statistics bureaus, and academic studies.^[
Those who are interested can access and explore these data on the web at <https://fsolt.org/swiid/swiid_source.html>.]

```{r}
#| label: data_by_method
#| echo: false
#| fig-cap: "\\label{fig:data_by_method}Income Inequality Observations by Method of Collection"
#| fig-height: 2
#| fig-width: 4


swiid_source %>% 
  count(mode) %>% 
  mutate(method = fct_recode(factor(mode), 
                             "API" = "api",
                             "Spreadsheet\nDownload" = "sheet",
                             "Web/PDF\nScrape" = "scrape",
                             "Hand\nEntered" = "hand") %>% 
           fct_relevel("Hand\nEntered",
                       "Web/PDF\nScrape",
                       "Spreadsheet\nDownload",
                       "API")) %>% 
  ggplot(aes(x = n, y = method, fill = method)) +
  geom_bar(stat="identity") +
  theme_bw() +
  theme(axis.title.y = element_blank()) +
  xlab("Observations") +
  scale_fill_manual("Legend", values = c("Hand\nEntered" = "gray75",
                                         "Web/PDF\nScrape" = "gray10",
                                         "Spreadsheet\nDownload" = "gray10",
                                         "API" = "gray10")) + 
  theme(legend.position="none")

# Hand-Entered and Machine-Collected
```

In early versions of the SWIID, all of these source data were entered by hand, and checks of newly entered observations revealed that error rates were high.
Moreover, many of the sources consulted frequently update or revise their figures.
To avoid data-entry errors and ensure that updates and revisions are automatically incorporated as they become available, since 2015 the process of collecting the source data has been automated to the greatest extent practicable [@Solt2020, 1184-1185].^[
The R code for this automated data collection can be viewed here: <https://github.com/fsolt/swiid/blob/master/R/data_setup.R>.]

Most international organizations and a few national statistical bureaus use application programming interfaces (APIs) that facilitate automating the inclusion of their data; the R community has often built packages using these APIs to make the task even easier [see @Blondel2018; @Magnusson2014; @Lahti2017; @Lugo2017; @Wickham2018].
The SWIID takes as much advantage of these resources as possible, as shown in Figure&nbsp;\ref{fig:data_by_method}.
Although the sources with APIs are relatively few, they contain by far the most data: `r api_percent`% of the observations are collected in this way.
When no API is available, the automation script downloads and reads any available spreadsheets [see @Wickham2016a].
In the absence of a spreadsheet, the process of scraping the data either directly from the web or, preferably, from a pdf file [see @Sepulveda2024] is automated.
Together the collection of `r automated_percent`% of the source data is scripted.
This means not only that the possibility of errors introduced by hand entry for a vast majority of observations is eliminated but also that the updates and revisions that are frequent in these data are automatically incorporated as they become available.

However, it also means that some `r 100 - automated_percent`% of the observations are entered by hand.^[
The resulting spreadsheet can be found at <https://github.com/fsolt/swiid/blob/master/data-raw/fs_added_data.csv>.]
Many sources contain just a handful or fewer observations, making the payoff to the often laborious process of data cleaning too small to justify the effort.
Some sources---including most academic articles---are behind paywalls, making automation particularly challenging.
When these sources contain more than a handful of observations, these are still collected using Sepulveda's [-@Sepulveda2024] `tabulapdf` R package to avoid data-entry errors.
Other sources, such as many books, cannot be read directly into R.
And finally, one source contains crucial information encoded in the typeface of its tables [see @Mitra2006, 6]; this information would be lost if the tables were read directly into R.
All such new observations are entered twice into separate spreadsheets.
Most often this has been done by two different investigators, but sometimes sequentially by a single researcher.
Either way, using this double-entry method allows for automated cross-checks of the newly entered data that increase the chances that errors are identified and corrected [see @Barchard2011].

To summarize, the process of collecting the source data for the SWIID is 90% automated, and the dual-entry method is employed for the remaining 10%.
The upshot of this process is that the SWIID's harmonized estimates of income inequality are reliable, frequently updated, and employed around the world by international organizations, central banks, and other researchers in academia and beyond.


# Wrangling Public Opinion Data for Harmonization

Scholarship on comparative public opinion only rarely benefits from relevant items asked annually by the same survey in many countries [see, e.g., @Hagemann2017].
To address the lack of cross-national and longitudinal data on many topics, a number of works have presented latent variable models that harmonize available but incomparable survey items [see @Caughey2019; @Claassen2019; @McGann2019; @Solt2020c; @Kolczynska2024].
This approach has been used to generate cross-national time-series measures of public opinion on a range of topics, from economic, social, and immigration conservatism [@Caughey2019] to trust in government [@Kolczynska2024].
The Dynamic Comparative Public Opinion (DCPO) model presented in @Solt2020c in particular has been employed to measure gender egalitarianism [@Woo2023], political interest [@Hu2024], and support for gay rights [@Woo2025], among other aspects of public opinion (see <https://dcpo.org/>).

This sort of work is by nature extremely data intensive, drawing on information extracted from dozens if not hundreds of survey datasets.
In DCPO projects, the collection of the source data proceeds in two steps: first, identifying the relevant survey items, and second, accumulating the survey responses.

The first step, identifying the relevant survey items, involves finding surveys that contain questions on the topic of interest and then recording in a spreadsheet the survey, the variable representing the question of interest in the dataset, the question text, the response values ordered from least to most of the concept being investigated, and the original textual response categories.
At present, this step is done entirely by hand.
To minimize data-entry errors, the dual-entry method is used here too.
Multiple collaborators go through the process separately, and the resulting spreadsheets are compared to catch the omnipresent data-entry errors.

The second step, accumulating the survey responses, is fully automated through the use of the `DCPOtools` R package [@Solt2018].
When passed the spreadsheet created in the first step, this package reads in each recorded survey dataset, extracts the variable of interest, reorders the response values for this variable from least to most of the concept investigated, applies survey weights, and then aggregates the weighted number of respondents in each of the reordered response categories in each country for each year in which the survey was fielded.
`DCPOtools` also automatically ensures that country names are standardized using the excellent `countrycode` package for R [@Arel-Bundock2018] and that the years accurately reflect actual fieldwork dates using internal crosswalk tables.
The aggregated number of respondents for each observed response-item-country-year then serve as the source data for the latent variable model.

To illustrate the advantages of this more robust process relative to manual entry, we reexamine the source data for latent variable estimates of public support for democracy that were used first in a pair of very prominent publications [see @Claassen2020a; @Claassen2020b] and subsequently in additional studies [see @Claassen2022; @Jacob2025].
These source data consist of thousands of nationally aggregated responses to dozens of different questions asked in cross-national survey projects that were apparently copied by hand into a spreadsheet from the crosstabulations included in the surveys' codebooks [see @Claassen2020d].
Two pieces of data were collected for each distinct survey item in each country and year it was asked: the number of respondents to give a democracy-supporting response---defined, for ordinal questions, as those above the median value of the response scale [@Claassen2020b, Appendix 1.3]---and the total number of respondents to whom the question was posed.

We re-collected all of these source data from the original surveys.
Following the two-step process described above, for each of the 52 survey items in the source data, we first identified the variables, response categories, and so on, and then we used `DCPOtools` to collect the needed data from the survey datasets by machine.
In @fig-comparison, we compare the percentage of respondents to give a democracy-supporting response in the original hand-entered spreadsheet with the percentage in the machine-collected data.
When points fall along the plot's dotted line, it indicates that the hand-entered and machine-collected source data report the same percentages.
Points above this diagonal represent observations for which the hand-entered data are higher than the percentage calculated directly from the survey datasets, while points below this line are observations where the hand-entered data are lower than this machine-collected percentage.

```{r}
#| label: data_comparison
#| include: false


if (!file.exists(here::here("data", "supdem raw survey marginals.csv"))) {
  tempfile <- dataverse::get_file_by_doi("10.7910/DVN/HWLW0J/RA8IJC",
                                         server = "dataverse.harvard.edu") # AJPS replication file, not included in APSR replication

  writeBin(tempfile, here::here("data", "supdem raw survey marginals.csv"))
  rm(tempfile)
}

# publication data
sd <- read_csv(here::here("data", "supdem raw survey marginals.csv"), col_types = "cdcddcdc") %>% 
  mutate(item_fam = str_extract(tolower(Item), "^[a-z]+"),
         country = countrycode::countrycode(Country, "country.name", "country.name"),
         dataset = "supdem") %>% 
  rename(year = Year, project = Project) %>% 
  with_min_yrs(2) # Selecting data w. at least two years

# corrected data
if (!file.exists(here::here("data", "claassen_input_raw.csv"))) {
  claassen_input_raw <- DCPOtools:::claassen_setup(
    vars = read_csv(here::here("data", "mood_dem.csv"),
                    col_types = "cccccc")) %>% 
    pluck("lower")
  
  write_csv(claassen_input_raw, 
            file = here::here("data",
                              "claassen_input_raw.csv"))
}

claassen_input_raw <- read_csv(here::here("data",
                                          "claassen_input_raw.csv"),
                               col_types = "cdcddcd")

claassen_input_raw1 <- claassen_input_raw %>%
  filter(!((
    str_detect(item, "army_wvs") &
      # WVS obs identified as problematic by Claassen
      ((country == "Albania" & year == 1998) |
         (country == "Indonesia" &
            (year == 2001 | year == 2006)) |
         (country == "Iran" & year == 2000) |
         (country == "Pakistan" &
            (year == 1997 | year == 2001)) | # 1996 in Claassen
         (country == "Vietnam" & year == 2001)
      )
  ) |
    (
      str_detect(item, "strong_wvs") &
        ((country == "Egypt" & year == 2012) |
           (country == "Iran" &
              (year == 2000 | year == 2007)) | # 2005 in Claassen
           (country == "India") |
           (country == "Pakistan" &
              (year == 1997 | year == 2001)) | # 1996 in Claassen
           (country == "Kyrgyzstan" &
              (year == 2003 | year == 2011)) |
           (country == "Romania" &
              (year == 1998 | year == 2005 | year == 2012)) |
           (country == "Vietnam" & year == 2001)
        )
    ) |
    (
      country %in% c(
        "Puerto Rico",
        "Northern Ireland",
        "SrpSka Republic",
        "Hong Kong SAR China"
      )
    ))) %>%
  with_min_yrs(2)

claassen_input0 <- DCPOtools::format_claassen(claassen_input_raw1)

cri <- claassen_input0$data %>% 
  mutate(p_dcpo = str_extract(survey, "^[a-z]+"), 
         project = case_when(p_dcpo == "afrob" ~ "afb",
                             p_dcpo == "amb" ~ "lapop",
                             p_dcpo == "arabb" ~ "arb",
                             p_dcpo == "asiab" ~ "asb",
                             p_dcpo == "asianb" ~ "asnb",
                             p_dcpo == "neb" ~ "ndb",
                             p_dcpo == "sasianb" ~ "sab",
                             TRUE ~ p_dcpo),
         item_fam = str_extract(item, "^[a-z]+"),
         item_fam = if_else(item_fam == "election", "elec", item_fam),
         dataset = "cri")

supdem_cri <- full_join(sd, cri, by = c("country", "year", "item_fam", "project"))

# While DCPO assigns actual year of survey, Claassen (2020) often uses nominal year
# of wave, so some corrections are required to match observations (~8% of obs);
# we could write about this as a data-entry error, too, but no space to spare,
# so we'll adopt those years instead to facilitate straight-up comparison
no_problems <- inner_join(sd %>% select(-dataset), 
                          cri %>% select(-dataset),
                          by = c("country", "year", "item_fam", "project")) %>% 
  mutate(diff_year = 0) # 3403 obs

needed <- anti_join(sd %>% select(-dataset),
                    cri %>% select(-dataset))                   # 313 obs

available <- anti_join(cri %>% select(-dataset),
                       sd %>% select(-dataset))                # 1310 obs

year_fixes <- left_join(needed,    # 304 obs
                        available,
                        by = c("country", "project", "item_fam")) %>% 
  mutate(diff = year.x - year.y) %>% 
  group_by(country, project, item_fam, year.x) %>% 
  mutate(closest_to_claassen = min(abs(diff))) %>% 
  ungroup() %>% 
  group_by(country, project, item_fam, year.y) %>% 
  mutate(closest_to_dcpo = min(abs(diff))) %>% 
  ungroup() %>% 
  filter(closest_to_claassen == abs(diff) & closest_to_dcpo == abs(diff) & abs(diff) <= 3) %>% 
  filter(!(country == "Egypt" & year.x == 2014 & survey == "afrob5")) %>%  # double matches (it's really afrob6)
  mutate(year = year.y,
         diff_year = abs(diff))

cys_crosswalk <- year_fixes %>% 
  select(country, y_dcpo = year.y, y_claassen = year.x, survey) %>% 
  distinct()

claassen_input_raw2 <- claassen_input_raw1 %>%
  # left_join(cys_crosswalk, by = c("country", "year" = "y_dcpo", "survey")) %>% 
  # mutate(year = if_else(is.na(y_claassen), year, y_claassen),
  #        p_dcpo = str_extract(survey, "^[a-z]+"), 
  #        project = case_when(p_dcpo == "afrob" ~ "afb",
  #                            p_dcpo == "amb" ~ "lapop",
  #                            p_dcpo == "arabb" ~ "arb",
  #                            p_dcpo == "asiab" ~ "asb",
  #                            p_dcpo == "asianb" ~ "asnb",
  #                            p_dcpo == "neb" ~ "ndb",
  #                            p_dcpo == "sasianb" ~ "sab",
  #                            TRUE ~ p_dcpo)) %>% 
  # right_join(sd %>% 
  #             distinct(country, year, project),
  #           by = c("country", "year", "project")) %>% 
  right_join(sd %>% distinct(country), by = "country")

claassen_input <- DCPOtools:::format_claassen(claassen_input_raw2)

cri2 <- claassen_input$data %>% 
  mutate(p_dcpo = str_extract(survey, "^[a-z]+"), 
         project = case_when(p_dcpo == "afrob" ~ "afb",
                             p_dcpo == "amb" ~ "lapop",
                             p_dcpo == "arabb" ~ "arb",
                             p_dcpo == "asiab" ~ "asb",
                             p_dcpo == "asianb" ~ "asnb",
                             p_dcpo == "neb" ~ "ndb",
                             p_dcpo == "sasianb" ~ "sab",
                             TRUE ~ p_dcpo),
         item_fam = str_extract(item, "^[a-z]+"),
         item_fam = if_else(item_fam == "election", "elec", item_fam),
         dataset = "cri2")

supdem_cri2 <- full_join(sd, 
                         cri2, 
                         by = c("country", "year", "item_fam", "project"))

no_problems2 <- inner_join(sd %>% select(-dataset), 
                          cri2 %>% select(-dataset),
                          by = c("country", "year", "item_fam", "project")) %>% 
  mutate(year_correct = TRUE) # 3706 obs

needed2 <- anti_join(sd %>% select(-dataset),
                    cri2 %>% select(-dataset))                   # 10 obs; these are not in the surveys

data_comparison <- bind_rows(no_problems, year_fixes) %>% 
  mutate(perc_cls = Response/Sample*100,
         perc_ours = x/samp*100,
         diff_perc = perc_cls - perc_ours,
         diff_perc_abs = abs(diff_perc),
         diff_x = round(Response - x),
         diff_samp = round(Sample - samp)) %>% 
  select(-CAbb, -COWCode)

save(data_comparison, file = here::here("data", "data_comparison.rda"))
```

```{r}
#| label: fig-comparison
#| fig-cap: "Comparing Democracy-Supporting Responses in Hand-Entered and Machine-Collected Data"
#| fig-align: "center"
#| fig-height: 5

load(here::here("data", "data_comparison.rda"))

bad_items <- c("Asia Barometer:\nEvaluation of Democracy",
               "Asian Barometer II:\nSuitability of Democracy",
               "Pew Global Attitudes:\nImportance of Democracy",
               "Other Items")

plot_support <- data_comparison %>% 
  mutate(p_dcpo3 = case_when(Item == "EvDemoc_asb" ~ "Asia Barometer:\nEvaluation of Democracy",
                             Item == "DemocSuit_asnb" & dplyr::between(year, 2005, 2008) ~ "Asian Barometer II:\nSuitability of Democracy",
                             Item == "ImpDemoc_pew" ~ "Pew Global Attitudes:\nImportance of Democracy",
                             TRUE ~ "Other Items") %>% 
           factor(levels = bad_items)) %>% 
  ggplot(aes(x = perc_ours, y = perc_cls)) +
    geom_point(aes(color = p_dcpo3, shape = p_dcpo3), size = 1.5) +
    labs(y = "Hand-Entered Data",
         x = "Machine-Collected Data") + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  theme_bw() +
  theme(legend.justification=c(1,0), 
        legend.position=c(.99,.01),
        legend.text = element_text(size = 6),
        legend.background = element_rect(fill = "transparent"),
        legend.key.size = unit(1.1, 'lines')) +
  scale_colour_manual(name = element_blank(),
                        values = alpha(c("gray50",
                                         "gray10",
                                         "gray35",
                                         "gray50"),
                                       c(1, 1, 1, .3)),
                        breaks = bad_items) +
  scale_shape_discrete(name = element_blank(),
                       breaks = bad_items)

tx_note <- str_wrap("Notes: Each point represents the percentage of respondents in a country-year to give a democracy-supporting response to a particular survey item.  Hand-entered data is as reported in Claassen (2020c); the machine-collected data was collected directly from the original surveys.  The Asia Barometer's item on the evaluation of democracy accounts for most overreports, and the Pew Global Attitudes item on the importance of democracy accounts for most substantial underreports.  In both cases, as well as the overreports of the suitability of democracy item in the second wave of the Asian Barometer, the issues can be easily explained by errors in transcribing the data in accordance with the reported coding rule.  Deviations in other items result from inconsistent treatment of missing data and/or survey weights, reflecting in part differences in codebook reporting practices across surveys.",
                    width = 96)

plot_support + 
    patchwork::plot_annotation(caption = tx_note) &
    theme(plot.caption = element_text(hjust = 0))

```

For `r {mean(data_comparison$diff_perc_abs < .5)*100} %>% round()`% of the country-year-item observations, the difference between these percentages was negligible---less than half a percent---yielding points approximately along the plot's dotted line.
But for the remaining observations, the difference was often substantial due to data-entry errors in the hand-entered data.
For three survey items contributing observations to a total of 135 country-years, the differences were found to result from deviations from the original study's coding rules: for two items the median category was mistakenly also counted as a democracy-supporting response, and for one an above-median category was not (for details, see online Appendix A).

While these issues involve mistakes in recording the numerator of the percentage, the number of respondents who provided a democracy-supporting answer, entering the denominator, the total number of respondents asked a question, was also problematic on occasion.
For example, when the Americas Barometer surveyed Canada in 2010, it asked half its sample, whether when "democracy doesn't work," Canadians "need a strong leader who doesn’t have to be elected through voting."
Those who were not asked the question were included in the total number of respondents.
According to the study's coding rules, refusing to answer is equivalent to answering in a fashion not supporting democracy [see @Claassen2020b, Appendix 1.3].
This rule may or may not be a reasonable coding choice [see @Hu2025], but including in this category those who were never asked the question at all is clearly a data-entry error.

Another source of data-entry errors here involves survey weights.
Weighting raw survey results to maximize the extent to which they are representative of the target population is important.
Relying on toplines reported in codebooks rather than the survey data itself evidently caused some mistakes in correctly entering the needed information here, as codebooks do not always take survey weights into account.
These errors shifted the percentage of democracy-supporting responses in both directions, typically by relatively small amounts.

Finally, although not depicted on this plot, data-entry errors were also evident in the variable recording the year in which a survey was conducted: these typically reflected differences between the nominal year of a survey wave and when the survey was actually in the field in a particular country.
This was an issue for some `r {nrow(needed)/nrow(sd) * 100} %>% round()`% of the country-year-item observations.

Any one of these mistakes should give pause to any researchers tempted to meet the challenge of 'janitor work' in harmonization research by hand entering the needed data.
Correcting all of them is sufficient to undermine the results published in @Claassen2020b on the sources of democratic support (see online Appendix B).


# Discussion

Harmonization projects like those discussed above are data-intensive efforts, and data-wrangling 'janitor work' is often a substantial part of the research process.
This means data-entry errors are particularly dangerous to these undertakings.
The threat is a subtle one that is not easily detected.
To discern it requires close scrutiny of every manual entry; merely examining the data and their distribution will uncover few errors [@Barchard2011, 1837-1838].

Our three recommendations---to maximize automation, use the double-entry method, and embrace teamwork---like similar open-science prescriptions, undoubtedly take effort [see @Engzell2021].
But as researchers automate more of their data entry, the chances that they can reuse their code in subsequent projects improve; moreover, as the examples of the API packages used by the SWIID and `DCPOtools` demonstrate, many common janitor-work chores already have been packaged as open-source software to make researchers' task even easier and more straightforward. 
<!-- For example, see `readtext` [@Benoit2021] for formatting text files. -->
<!-- "Write code instead of working by hand," as @Christensen2019 [, 197] admonish, "don't use Microsoft Excel if it can be avoided." -->
And while the double-entry method is labor intensive, experiments have shown that it reduces error rates by thirty-fold even when done immediately after the initial collection and by the same person [@Barchard2011, 1837]; this payoff justifies the extra effort.
Teamwork cuts the other way.
For any project involving entering data by hand, splitting the task up among team members lessens the probability of errors due to fatigue arising in the first place, and coupled with the double-entry method, allows discrepancies to be noted, discussed, and resolved correctly.
Having two sets of eyes on complex materials such as survey codebooks also increases the chances that nuances of the presentation like survey weights will be uncovered.

Data-entry errors are inevitable, and even following these recommendations is unlikely to eliminate them entirely.
Nonetheless, with careful attention, not only can the threat of data-entry errors to our 'janitor work', our efforts at data harmonization, and our understanding of the world be minimized, but the transparency, openness, and credibility of our research can continue to grow.

\newpage
\theendnotes

# References {.unnumbered}

::: {#refs-main}
:::

\pagebreak

\hypertarget{appendix-appendix}{%
\appendix}

# Online Supplementary Materials {.unnumbered}

\setcounter{page}{1}
\renewcommand{\thepage}{A\arabic{page}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}

# Details on Data-Entry Problems in the Democratic Support Source Data
Comparing the original hand-entered dataset [@Claassen2020d] with data that were machine-collected using the `DCPOtools` package for R [@Solt2018] revealed three survey items for which the hand-entered data did not match the data's documented coding rules.
These rules indicate that responses above the median value in the response scale are to be considered as supporting democracy, while those at the median value and below are not [see @Claassen2020b, Appendix 1.3].

First, the Asia Barometer asked respondents in 35 country-years to indicate whether they thought "a democratic political system" would be very good, fairly good, or bad for their country.
According to the original study's coding rules [see @Claassen2020b, Appendix 1.3], only answers above the median of the response categories should be considered as democracy supporting, yet in this case the lukewarm intermediate category was coded as supporting democracy as well.
Similarly tepid responses at and below the median response category to similar questions (e.g., in the Arab Barometer, that democracy was "somewhat appropriate" for the country) were coded as not supportive, confirming that this is indeed a data-entry error.
This discrepancy resulted in hand-entered percentages of democracy-supporting responses ranging from `r data_comparison %>% filter(item=="evdemoc_asiab") %>% pull(diff_perc) %>% min() %>% round()` to `r data_comparison %>% filter(item=="evdemoc_asiab") %>% pull(diff_perc) %>% max() %>% round()` percentage points higher than the data automatically collected directly from the survey datasets.
<!-- and averaging `r data_comparison %>% filter(item=="evdemoc_asiab") %>% pull(diff_perc) %>% mean() %>% round()` points higher. -->

Second, the four waves of the Asian Barometer included the following item: "Here is a similar scale of 1 to 10 measuring the extent to which people think democracy is suitable for our country. If 1 means that democracy is completely unsuitable for [name of country] today and 10 means that it is completely suitable, where would you place our country today?"
In accordance with the coding rules of the study, responses of 6 through 10 are considered democracy supporting, and that is how the first, third, and fourth waves of the survey are coded.
For the second wave, however, 5 was erroneously also included among the democracy-supporting responses.
This data-entry error resulted percentages overstated by as much as `r data_comparison %>% filter(item=="democsuit_asianb" & (year==2005|year==2006|year==2008)) %>% pull(diff_perc) %>% max() %>% round()` percentage points in 9 country-years. 

And third, the Pew Global Attitudes surveys' four-point item asking about the importance of living in a country with regular and fair contested elections: the question wording is "How important is it to you to live in a country where honest elections are held regularly with a choice of at least two political parties? Is it very important, somewhat important, not too important or not important at all?"
In this case, rather than including respondents who gave both responses above the median---"very important" and "somewhat important"---only those respondents who answered "very important" were entered as supporting democracy.
This error caused the hand-entered percentages to be substantially lower in 91 country-years.

# Consequences for Inference of Democratic Support Data-Entry Errors

```{r corrected_cm5, eval=FALSE, cache=FALSE, include=FALSE, results=FALSE}
iter <- 500

cm5 <- cmdstan_model("R/supdem.stan.mod5.stan")
corrected_output <- cm5$sample(
    data = claassen_input[1:11], 
    max_treedepth = 11,
    adapt_delta = 0.99,
    step_size = 0.02,
    seed = 324, 
    chains = 4, 
    parallel_chains = 4,
    iter_warmup = iter/2,
    iter_sampling = iter/2,
    refresh = iter/50
)
results_path <- here::here(file.path("data", 
                                     "corrected", 
                                     {str_replace_all(Sys.time(), "[- :]", "") %>%
                                         str_replace("\\d{2}$", "")}))
dir.create(results_path, 
           showWarnings = FALSE, 
           recursive = TRUE)
corrected_output$save_data_file(dir = results_path,
                           random = FALSE)
corrected_output$save_output_files(dir = results_path,
                              random = FALSE)
```

```{r corrected_cm5_results, cache=TRUE}
# if (!exists("results_path")) {
#   latest <- "202205211148"
#   results_path <- here::here("data", "corrected", latest)
#   
#   # Define OSF_PAT in .Renviron: https://docs.ropensci.org/osfr/articles/auth
#   if (!file.exists(file.path(results_path, paste0("supdem.stan.mod5-", latest, "-1.csv")))) {
#     dir.create(results_path, showWarnings = FALSE, recursive = TRUE)
#     osf_retrieve_node("vmxkn") %>%
#       osf_ls_files() %>% 
#       filter(name == latest) %>% 
#       osf_download(path = here::here("data", "corrected"))
#   }
# }

latest <- "202206151125"
results_path <- here::here("data", "corrected", latest)

corrected_output <- as_cmdstan_fit(here::here(results_path,
                                         list.files(results_path, pattern = "csv$")))

```

```{r corrected_cm5_results_summary, cache=TRUE}
summarize_cm5_results <- function(cm5_input,
                             cm5_output,
                             pars = c("theta"),
                             probs = c(.1, .9)) {

    question <- country <- year <- parameter <- variable <- kk <- tt <- qq <- rr <- NULL

    dat <- cm5_input$data

    qcodes <- dat %>%
        dplyr::transmute(item = item,
                         qq = as.numeric(as.factor(item))) %>% 
        unique()

    kcodes <- dat %>%
        dplyr::transmute(country = country,
                         kk = as.numeric(as.factor(country))) %>% 
        unique()

    tcodes <- dat %>%
        dplyr::transmute(year = year,
                         tt = as.numeric(as.factor(year))) %>% 
        unique() %>% 
        arrange(year)


    ktcodes <- dat %>%
        dplyr::group_by(country) %>%
        dplyr::summarize(first_yr = min(year),
                         last_yr = max(year))

    if ("R6" %in% class(cm5_output)) {
        fit <- cm5_output

        summary_measures <- c("mean", "median", "sd", "mad", "rhat", "ess_bulk", "ess_tail")

        res <- map_df(pars, function(par) {
            if (par == "theta") {
                fit$summary("theta",
                            ~posterior::quantile2(., probs = probs),
                            summary_measures) %>%
                    dplyr::mutate(tt = as.numeric(gsub("theta\\[(\\d+),\\d+\\]",
                                                       "\\1",
                                                       variable)),
                                  kk = as.numeric(gsub("theta\\[\\d+,(\\d+)\\]",
                                                       "\\1",
                                                       variable))) %>%
                    dplyr::left_join(kcodes, by = "kk") %>%
                    dplyr::left_join(tcodes, by = "tt") %>%
                    dplyr::left_join(ktcodes, by = "country") %>%
                    dplyr::filter(year >= first_yr & year <= last_yr) %>%
                    dplyr::arrange(kk, tt) %>%
                    dplyr::select(country, year,
                                  starts_with("me"),
                                  sd, mad, starts_with("q"),
                                  rhat, starts_with("ess"),
                                  variable, kk, tt)
            } else if (par == "sigma") {
                fit$summary("sigma",
                            ~posterior::quantile2(., probs = probs),
                            summary_measures) %>%
                    dplyr::mutate(tt = as.numeric(gsub("sigma\\[(\\d+),\\d+\\]",
                                                       "\\1",
                                                       variable)),
                                  kk = as.numeric(gsub("sigma\\[\\d+,(\\d+)\\]",
                                                       "\\1",
                                                       variable))) %>%
                    dplyr::left_join(kcodes, by = "kk") %>%
                    dplyr::left_join(tcodes, by = "tt") %>%
                    dplyr::mutate(year = if_else(tt == 1,
                                                 as.integer(year),
                                                 as.integer(min(year, na.rm = TRUE) + tt - 1))) %>%
                    dplyr::left_join(ktcodes, by = "country") %>%
                    dplyr::filter(year >= first_yr & year <= last_yr) %>%
                    dplyr::arrange(kk, tt) %>%
                    dplyr::select(country, year,
                                  starts_with("me"),
                                  sd, mad, starts_with("q"),
                                  rhat, starts_with("ess"),
                                  variable, kk, tt)
            } else if (par == "alpha") {
                fit$summary("alpha",
                            ~posterior::quantile2(., probs = probs),
                            summary_measures) %>%
                    dplyr::mutate(qq = as.numeric(gsub("alpha\\[(\\d+)]",
                                                       "\\1",
                                                       variable))) %>%
                    dplyr::left_join(qcodes, by = "qq") %>%
                    dplyr::arrange(qq) %>%
                    dplyr::select(question, n, everything())
            } else if (par == "beta") {
                fit$summary("beta",
                          ~posterior::quantile2(., probs = probs),
                          summary_measures) %>%
                    dplyr::mutate(rr = as.numeric(gsub("beta\\[(\\d+),\\d+\\]",
                                                       "\\1",
                                                       variable)),
                                  qq = as.numeric(gsub("beta\\[\\d+,(\\d+)\\]",
                                                       "\\1",
                                                       variable)))%>%
                    dplyr::left_join(qcodes, by = "qq") %>%
                    dplyr::arrange(qq, rr) %>%
                    dplyr::filter(rr <= rr_max) %>%
                    dplyr::select(question, n, everything())
            } else if (par == "delta") {
                fit$summary("delta",
                            ~posterior::quantile2(., probs = probs),
                            summary_measures) %>%
                    dplyr::mutate(qq = as.numeric(gsub("delta\\[(\\d+),\\d+\\]",
                                                       "\\1",
                                                       parameter)),
                                  kk = as.numeric(gsub("delta\\[\\d+,(\\d+)\\]",
                                                       "\\1",
                                                       parameter)))%>%
                    dplyr::left_join(qcodes, by = "qq") %>%
                    dplyr::left_join(kcodes, by = "kk") %>%
                    dplyr::arrange(qq) %>%
                    dplyr::select(question, n, country, everything())
            }
        })
    } else {
        res <- map_df(pars, function(par) {
            if (par == "theta") {
                rstan::summary(dcpo_output, pars = "theta", probs = probs) %>%
                    dplyr::first() %>%
                    as.data.frame() %>%
                    tibble::rownames_to_column("parameter") %>%
                    tibble::as_tibble() %>%
                    dplyr::mutate(tt = as.numeric(gsub("theta\\[(\\d+),\\d+\\]",
                                                       "\\1",
                                                       parameter)),
                                  kk = as.numeric(gsub("theta\\[\\d+,(\\d+)\\]",
                                                       "\\1",
                                                       parameter))) %>%
                    dplyr::left_join(kcodes, by = "kk") %>%
                    dplyr::left_join(tcodes, by = "tt") %>%
                    dplyr::mutate(year = if_else(tt == 1,
                                                 as.integer(year),
                                                 as.integer(min(year, na.rm = TRUE) + tt - 1))) %>%
                    dplyr::left_join(ktcodes, by = "country") %>%
                    dplyr::filter(year >= first_yr & year <= last_yr) %>%
                    dplyr::arrange(kk, tt) %>%
                    dplyr::select(-first_yr, -last_yr)
            } else if (par == "sigma") {
                rstan::summary(dcpo_output, pars = "sigma", probs = probs) %>%
                    dplyr::first() %>%
                    as.data.frame() %>%
                    tibble::rownames_to_column("parameter") %>%
                    tibble::as_tibble() %>%
                    dplyr::mutate(tt = as.numeric(gsub("sigma\\[(\\d+),\\d+\\]",
                                                       "\\1",
                                                       parameter)),
                                  kk = as.numeric(gsub("sigma\\[\\d+,(\\d+)\\]",
                                                       "\\1",
                                                       parameter))) %>%
                    dplyr::left_join(kcodes, by = "kk") %>%
                    dplyr::left_join(tcodes, by = "tt") %>%
                    dplyr::arrange(kk, tt)
            } else if (par == "alpha") {
                rstan::summary(dcpo_output, pars = "alpha", probs = probs) %>%
                    dplyr::first() %>%
                    as.data.frame() %>%
                    tibble::rownames_to_column("parameter") %>%
                    tibble::as_tibble() %>%
                    dplyr::mutate(qq = as.numeric(gsub("alpha\\[(\\d+)]",
                                                       "\\1",
                                                       parameter))) %>%
                    dplyr::left_join(qcodes, by = "qq") %>%
                    dplyr::arrange(qq)
            } else if (par == "beta") {
                rstan::summary(dcpo_output, pars = "beta", probs = probs) %>%
                    dplyr::first() %>%
                    as.data.frame() %>%
                    tibble::rownames_to_column("parameter") %>%
                    tibble::as_tibble() %>%
                    dplyr::mutate(rr = as.numeric(gsub("beta\\[(\\d+),\\d+\\]",
                                                       "\\1",
                                                       parameter)),
                                  qq = as.numeric(gsub("beta\\[\\d+,(\\d+)\\]",
                                                       "\\1",
                                                       parameter)))%>%
                    dplyr::left_join(qcodes, by = "qq") %>%
                    dplyr::arrange(qq, rr)
            } else if (par == "delta") {
                rstan::summary(dcpo_output, pars = "delta", probs = probs) %>%
                    dplyr::first() %>%
                    as.data.frame() %>%
                    tibble::rownames_to_column("parameter") %>%
                    tibble::as_tibble() %>%
                    dplyr::mutate(qq = as.numeric(gsub("delta\\[(\\d+),\\d+\\]",
                                                       "\\1",
                                                       parameter)),
                                  kk = as.numeric(gsub("delta\\[\\d+,(\\d+)\\]",
                                                       "\\1",
                                                       parameter)))%>%
                    dplyr::left_join(qcodes, by = "qq") %>%
                    dplyr::left_join(kcodes, by = "kk") %>%
                    dplyr::arrange(qq)
            }
        })
    }

    return(res)
}


theta_summary <- summarize_cm5_results(claassen_input,
                                       corrected_output,
                                       "theta")

if (!file.exists(here::here("data", "dem_mood_apsr.csv"))) {
    tempfile <- dataverse::get_file_by_doi(filedoi = "doi:10.7910/DVN/FECIO3/ACMGQG") # APSR replication
    
    writeBin(tempfile, here::here("data", "dem_mood_apsr.csv"))
    rm(tempfile)
}

df_apsr <- read_csv(here::here("data", "dem_mood_apsr.csv")) %>%
  left_join(theta_summary, by = c("Year" = "year", "Country" = "country"))
```

@Claassen2020b argued that public support for democracy moved thermostatically in response to changes in democracy, that is, that that changes in the latter prompt opposite changes in the former.
Here we show that correcting the data-entry errors we document yield results that provide no support for this conclusion.
After generating the latent variable of democratic support with these corrections, we replicated each of the models presented in @Claassen2020b exactly using both the original and the new version of the latent variable.
The results using the corrected data reveal that there is no evidence of a thermostatic relationship.

```{r regressionResult}
sd.plm <- pdata.frame(df_apsr, index = c("Country", "Year")) 
sd.plm2 <- pdata.frame(df_apsr %>% 
                         mutate(SupDem_trim = mean),
                       index = c("Country", "Year"))

ls_ivECM <- rep(c("diff(Libdem_z, lag = 1) + plm::lag(Libdem_z, 1)", 
                  "diff(Polyarchy_z, lag = 1) + plm::lag(Polyarchy_z, 1) + diff(Liberal_z, lag = 1) + plm::lag(Liberal_z, 1)"),
                each = 2) %>% 
  paste0(rep(c("",
               " + diff(Corrup_TI_z, lag = 1) + plm::lag(Corrup_TI_z, 1)"),
             times = 2))

ls_eqECM <- glue("diff(SupDem_trim, lag = 1) ~ plm::lag(SupDem_trim, 1:2) + {ls_ivECM} + diff(lnGDP_imp, lag = 1) + plm::lag(lnGDP_imp, 1)") %>% 
  as.vector()

ls_ivFD <- rep(c("Libdem_z",
                 "Polyarchy_z + Liberal_z"), 
               each = 2) %>% 
  paste0(rep(c("",
               " + Corrup_TI_z"),
             times = 2))

ls_eqFD <- glue("SupDem_trim ~ {ls_ivFD} + lnGDP_imp") %>% 
  as.vector()

ls_mod_original <- c(
  glue("plm({ls_eqECM}, model = 'pooling', data = sd.plm)"),
  glue("plm({ls_eqFD}, model = 'fd', index = 'Country', data = sd.plm)")
)
ls_mod_correct <- c(
  glue("plm({ls_eqECM}, model = 'pooling', data = sd.plm2)"),
  glue("plm({ls_eqFD}, model = 'fd', index = 'Country', data = sd.plm2)")
)

ls_mod <- c(ls_mod_original, ls_mod_correct)

result_clsAPSR <- map(ls_mod, ~eval(parse(text = .)))

result_clsAPSR_tidy <- map(result_clsAPSR, function(aResult) {
  tidy_result <- tidy(aResult, conf.int = TRUE) %>% 
    mutate(std.error = vcovHC_se(aResult))
  
  glance_result <- glance.plm(aResult)
  
  ls_result <- list(tidy_result, glance_result)
})
```

```{r regressionPlot, fig.cap= "The Effect of Democracy on Change in Public Support", fig.width=7, fig.height=8}

txt_models <- paste0("Model ",
                     rep(1:2, each = 4), ".", rep(1:4, 2),
                     " (",
                     rep(c("EC", "EC", "FD", "FD"), 2),
                     ")")

vec_numCoef <- map_dbl(c(1, 2, 5, 6, 3, 4, 7, 8), # ECM ECM FD FD
                       ~nrow(result_clsAPSR_tidy[[.]][[1]])) - 1 # subtract the intercepts

txt_model_lab <- map2(txt_models, vec_numCoef, ~rep(.x, times = .y)) %>% 
    unlist() # model lists
txt_model_lab <- c(txt_model_lab[str_which(txt_model_lab, "EC")], 
                   txt_model_lab[str_which(txt_model_lab, "FD")])
# Modify the orders to match the dataset

index_coefName <- c(
    `plm::lag(SupDem_trim, 1:2)1` = "Democratic~Support[t-1]",
    `plm::lag(SupDem_trim, 1:2)2` = "Democratic~Support[t-2]",
    `diff(Libdem_z, lag = 1)` = "Delta~Liberal~Democracy",
    `Libdem_z` = "Delta~Liberal~Democracy",
    `plm::lag(Libdem_z, 1)` = "Liberal~Democracy[t-1]",
    `diff(Polyarchy_z, lag = 1)` = "Delta~Electoral~Democracy",
    `Polyarchy_z` = "Delta~Electoral~Democracy",
    `plm::lag(Polyarchy_z, 1)` = "Electoral~Democracy[t-1]",
    `diff(Liberal_z, lag = 1)` = "Delta~Minoritarian~Democracy",
    `Liberal_z` = "Delta~Minoritarian~Democracy",
    `plm::lag(Liberal_z, 1)` = "Minoritarian~Democracy[t-1]",
    `diff(lnGDP_imp, lag = 1)` = "Delta~Log~GDP~per~capita",
    `lnGDP_imp` = "Delta~Log~GDP~per~capita",
    `plm::lag(lnGDP_imp, 1)` = "Log~GDP~per~capita[t-1]",
    `diff(Corrup_TI_z, lag = 1)` = "Delta~Corruption",
    `Corrup_TI_z` = "Delta~Corruption",
    `plm::lag(Corrup_TI_z, 1)` = "Corruption[t-1]"
)

df_plot <- map_dfr(result_clsAPSR_tidy, ~.[[1]]) %>% 
    filter(term != "(Intercept)") %>%
    mutate(
        type = ifelse(str_detect(term, "(_z|imp)$"), "FD", "EC"),
        term = index_coefName[term],
        term = factor(term, levels = unique(index_coefName)),
        submodel = rep(c("Published", "Corrected"), each = nrow(.)/2),
        submodel = factor(submodel, levels = c("Published","Corrected")),
        model = rep(txt_model_lab, times = 2),
        model = factor(model, levels = txt_models)
    )

txt_caption <- strwrap("Notes: Replications of Claassen (2020), Table 1, 47, and Table 2, 49.  Models denoted 'EC' are error-correction models; those marked 'FD' are first-difference models.", width = 115) %>% 
  paste0(sep="", collapse="\n")

small_multiple(df_plot, axis_switch = TRUE, dot_args = list(size = .9, fatten = 1)) +
    geom_hline(yintercept = 0, colour = "grey50", linetype = 2) +
    scale_color_grey(start = 0.8, end = 0.4, 
                     name = NULL,
                     breaks = c("Published",
                              "Corrected"),
                     labels = c("Publication Data",
                              "Corrected Data")) +
    theme(axis.text.x  = element_text(angle = 90, hjust = .5),
          strip.text.y.left = element_text(angle = 0),
          legend.position = c(0.006, -.01),
          legend.justification = c(1, 1), 
          legend.title = element_blank(),
          legend.title.align = 0.5,
          legend.text = element_text(size = 8),
          legend.background = element_rect(color="gray90"),
          legend.spacing = unit(-5, "pt"),
          legend.key.width = unit(4.5, "mm"),
          legend.key.height = unit(7, "mm"),
          plot.title.position = "plot",
          plot.caption.position = "plot",
          plot.caption = element_text(size = 10, hjust = 0)) +
  guides(color = guide_legend(override.aes = list(size=.3))) +
  ggtitle("Dependent Variable: Change in Public Democratic Support") +
  labs(caption = txt_caption)

# ggsave("images/results.png")
```

Figure \@ref(fig:regressionPlot) presents our results in a "small multiple" plot [@SoltHu2015] for a clear comparison of the coefficients of each variable in the article's models. 
In the plot, the dots represent point estimates and the whiskers show the associated 95% confidence intervals.
<!-- Each row depicts a variable's performance in its own scale across all of the models. -->
<!-- The lighter dots and whiskers replicate those reported in the published article; the darker ones are the estimates obtained with the corrected data. -->
Models 1.1 through 1.4, which replicate those presented in Table 1 of @Claassen2020b [, 47], examine the effects of overall liberal democracy using error-correction models and first-difference models.
As @Claassen2020b [, 46] notes, the thermostatic theory predicts that the estimated coefficient of the change in liberal democracy will be negative, while the classic theory suggests that lagged levels of liberal democracy will be positive.
When using the original publication data with their data-entry errors, we replicate the results of the article exactly: the coefficients estimated for the change in liberal democracy are large, negative, and statistically significant across all four models, just as the thermostatic theory predicts. 
The positive and statistically significant result for the lagged level of liberal democracy found in Model 1.1---supporting the classic theory---disappears when corruption is taken into account in Model 1.2, suggesting that "this effect is not particularly robust" [@Claassen2020b, 47].

When the data-entry errors are corrected, however, the results for these models suggest a very different set of conclusions.
The standard errors shrink across the board---indicating that the models are better estimated in the corrected data---but so do the magnitudes of the coefficients.
The positive and statistically significant result for the lagged level of liberal democracy remains in Model 1.1.
The estimate is only slightly smaller than in the publication data, and as with the publication data, it disappears when corruption is added in Model 1.2: the evidence, such as it is, for the classic theory, operationalized as a short-run process, remains substantively unchanged.
On the other hand, the estimates for the change in liberal democracy that provided support for the thermostatic theory are much smaller---very nearly exactly zero---and fail to reach statistical significance in any of these four models.
Models 2.1 through 2.4, which break liberal democracy into its electoral democracy and minoritarian democracy components, similarly undermine claims for the thermostatic theory.
The strong and statistically significant negative coefficients for the change in minoritarian democracy on public democratic support that are found using the publication dataset evaporate when the data-entry errors are corrected.
There is no support for the thermostatic theory.


<!-- # References {.unnumbered} -->

<!-- ::: {#refs-appendix} -->
<!-- ::: -->