---
format:
  pdf:
    number-sections: true
    papersize: a4
    keep-tex: false
    include-in-header: 
      text: |
        \usepackage{typearea}
        \usepackage{endnotes}
        \let\footnote=\endnote
crossref:
  sec-prefix: OSM
  sec-labels: alpha A
    
author:
  - name: Yue Hu
    affiliations:
      - ref: tsu
    orcid: 0000-0002-2829-3971
    email: yuehu@tsinghua.edu.cn
    url: https://www.drhuyue.site
  - name: Yuehong Cassandra Tai
    affiliations:
      - ref: psu
    orcid: https://orcid.org/0000-0001-7303-7443
    email: yhcasstai@psu.edu
  - name: Frederick Solt
    affiliations:
      - ref: ia
    orcid: 0000-0002-3154-6132
    email: frederick-solt@uiowa.edu
    url: https://www.fsolt.org
affiliations:
  - id: tsu
    name: Department of Political Science, Tsinghua University, Beijing, China
  - id: psu
    name: Center for Social Data Analytics, Pennsylvania State University, University Park, USA
  - id: ia
    name: Department of Political Science, University of Iowa, Iowa City, USA
thanks: "Corresponding author: [yuehong-tai@uiowa.edu](mailto:yuehong-tai@uiowa.edu). Current version: `r format(Sys.time(), '%B %d, %Y')`.  Replication materials and complete revision history may be found at [https://github.com/fsolt/wrangling_data](https://github.com/fsolt/wrangling_data). The authors contributed equally to this work.  Yue Hu appreciates the funding support from the National Natural Science Foundation of China (72374116) and Tsinghua University Initiative Scientific Research Program (2024THZWJC01)."
citeproc: false # to make multibib and wordcount work
filters:
  - authors-block
  - multibib # separate bib for main and appendix
  - at: pre-render
    path: "_extensions/andrewheiss/wordcount/wordcount.lua"
validate-yaml: false # for multibib to work
bibliography: 
    main: "main_wrangling.bib"
citation_package: natbib
csl: "american-political-science-review.csl"
tables: true # enable longtable and booktabs
fontsize: 12pt
indent: true
geometry: margin=1in
linestretch: 1.5 # double spacing using linestretch 1.5
colorlinks: false
link-citations: true
execute:
  echo: false
  message: false
  warning: false
  dpi: 600
editor_options: 
  chunk_output_type: console
editor:
  render-on-save: true
title: |
  | Best Practices for Getting
  | Past the 'Janitor Work'
  | Data Wrangling Before Harmonization:
# subtitle: "Preliminary version. Do not circulate without permission."
abstract: "This article focuses on a preliminary step in any ex-post data harmonization project---wrangling the pre-harmonized data---and suggests best practices for helping scholars avoid errors in this often-tedious work. To provide illustrations of these best practices, the article uses the examples of pre-harmonizing procedures used to produce the Standardized World Income Inequality Database (SWIID), a widely used database that uses Gini indices from multiple sources to create comparable estimates, and the Dynamic Comparative Public Opinion (DCPO) project, which creates a workflow for harmonizing aggregate public opinion data."
keywords: 
  - Data generation process
  - Machine assistance
  - Manual coding
  - Data cleaning
  - Data management
---

```{r}
#| label: setup
#| include: false

# options(repos = c(CRAN = "https://cloud.r-project.org"))

# if (!require(cmdstanr)) {
#   install.packages("cmdstanr", repos = c(
#     "https://mc-stan.org/r-packages/",
#     getOption("repos")
#   ))
#   library(cmdstanr)
#   install_cmdstan() # C++ toolchain required; see https://mc-stan.org/cmdstanr/articles/cmdstanr.html
# }

if (!require(pacman)) {
  install.packages("pacman")
}
library(pacman)

# p_install(janitor, force = FALSE)
# p_install_gh(c("fsolt/DCPOtools"), force = FALSE)

# load all the packages you will use below
p_load(
  # analysis
  # cmdstanr,
  # plm,
  # osfr,

  # presentation
  gridExtra,
  modelsummary,
  dotwhisker,
  latex2exp,

  # data wrangling
  DCPOtools,
  janitor,
  countrycode,
  here,
  broom,
  tidyverse,
  glue
)

theme_set(theme_minimal())
set.seed(313)

use("dplyr", c("filter", "select")) # require R 4.5.0

```

```{r}
#| label: preloadFuns

## Beck-Katz panel-corrected standard errors
vcovHC_se <- function(x) {
  plm::vcovHC(x, method = "arellano", cluster = "group") %>% # default setting
    diag() %>%
    sqrt()
}

## Tabulation -----------------------------------------------------------------------
na_types_dict <- list(
  "r" = NA_real_,
  "i" = rlang::na_int,
  "c" = NA_character_,
  "l" = rlang::na_lgl
)

### A function that converts a string to a vector of NA types.
### e.g. "rri" -> c(NA_real_, NA_real_, rlang::na_int)
parse_na_types <- function(s) {
  positions <- purrr::map(
    stringr::str_split(s, pattern = ""),
    match,
    table = names(na_types_dict)
  ) %>%
    unlist()

  na_types_dict[positions] %>%
    unlist() %>%
    unname()
}

### A function that, given named arguments, will make a one-row tibble, switching out NULLs for the appropriate NA type.
as_glance_tibble <- function(..., na_types) {
  cols <- list(...)

  if (length(cols) != stringr::str_length(na_types)) {
    stop(
      "The number of columns provided does not match the number of ",
      "column types provided."
    )
  }

  na_types_long <- parse_na_types(na_types)

  entries <- purrr::map2(
    cols,
    na_types_long,
    function(.x, .y) {
      if (length(.x) == 0) .y else .x
    }
  )

  tibble::as_tibble_row(entries)
}

tidy.pgmm <- function(x,
                      conf.int = FALSE,
                      conf.level = 0.95,
                      ...) {
  result <- summary(x)$coefficients %>%
    tibble::as_tibble(rownames = "term") %>%
    dplyr::rename(
      estimate = Estimate,
      std.error = `Std. Error`,
      statistic = `z-value`,
      p.value = `Pr(>|z|)`
    )

  if (conf.int) {
    ci <- confint(x, level = conf.level) %>%
      as.data.frame() %>%
      rownames_to_column(var = "term") %>%
      dplyr::rename(
        conf.low = `2.5 %`,
        conf.high = `97.5 %`
      )
    result <- dplyr::left_join(result, ci, by = "term")
  }

  result
}

glance.plm <- function(x, ...) {
  s <- summary(x)
  as_glance_tibble(
    nobs = stats::nobs(x),
    n.country = pdim(x)$nT$n,
    na_types = "ii"
  )
}

glance.pgmm <- function(x, ...) {
  s <- summary(x)
  as_glance_tibble(
    nobs = stats::nobs(x),
    n.country = pdim(x)$nT$n,
    n.inst = dim(x$W[[1]])[2],
    na_types = "iii"
  )
}

## dotwhisker::small_multiple() hacks

body(small_multiple)[[19]] <- substitute(
  p <-
    ggplot(
      df,
      aes(
        y = estimate,
        ymin = conf.low,
        ymax = conf.high,
        x = as.factor(model),
        colour = submodel
      )
    ) +
    do.call(geom_pointrange, point_args) +
    ylab("") +
    xlab("") +
    facet_grid(
      term ~ .,
      scales = "free",
      labeller = label_parsed,
      # enable LaTeX facet labels
      switch = "y"
    ) + # put facet labels on left
    scale_y_continuous(position = "right") # put axis label on right
)
```

\pagebreak

# A Wrangling Issue of Data Harmonization

Empowered by the spreading Internet and advancing computational power, researchers have entered an unprecedented age of data availability.
A growing volume of social science research aims to take the benefit to extend the generality: they employ large quantities of data drawn from different sources.
However, ensuring the quality of harmonized datasets remains a significant challenge in handling fitness to use and raw data quality monitoring among others [@Slomczynsi2025].
Beyond the focus on the harmonization process itself, we argue that quality assurance must begin *earlier* to the data-wrangling step where raw inputs are selected, processed, and prepared for harmonization.

The wrangling step determines the quality of data in the harmonization process, and the challenges is how to properly and transparently clean the increasing amount and diversity of data.
The conventional approach usually involves a notorious bulk of manual work on indicator identification, data merging, data scaling, and so on [see, e.g., @Lohr2014].
The tiresome task is easy to introduce errors in data collection procedure. 
Manual wrangling make a full reproducibility of research pipeline more difficulty and undermine the transparency [@Liu2019].

Here we provide a vivid example from real-life research to show how different results the seemingly basic, well-described hand-entered steps can produce.
We attempted to replicate the spreadsheets applied in prominent studies [@Claassen2020a; @Claassen2020b] following the replication instructions of the publications but with a more automatic process (elaborated below).
@fig-comparison presents democracy support percentages from both methods. 
Points on the diagonal show agreement; points above indicate higher hand-entered values, while points below show lower hand-entered percentages compared to machine-collected data.

```{r data-dcpo}
#| label: data_comparison
#| include: false


if (!file.exists(here::here("data", "supdem raw survey marginals.csv"))) {
  tempfile <- dataverse::get_file_by_doi("10.7910/DVN/HWLW0J/RA8IJC",
    server = "dataverse.harvard.edu"
  ) # AJPS replication file, not included in APSR replication

  writeBin(tempfile, here::here("data", "supdem raw survey marginals.csv"))
  rm(tempfile)
}

# publication data
sd <- read_csv(here::here("data", "supdem raw survey marginals.csv"), col_types = "cdcddcdc") %>%
  mutate(
    item_fam = str_extract(tolower(Item), "^[a-z]+"),
    country = countrycode::countrycode(Country, "country.name", "country.name"),
    dataset = "supdem"
  ) %>%
  rename(year = Year, project = Project) %>%
  with_min_yrs(2) # Selecting data w. at least two years

# corrected data
if (!file.exists(here::here("data", "claassen_input_raw.csv"))) {
  claassen_input_raw <- DCPOtools:::claassen_setup(
    vars = read_csv(here::here("data", "mood_dem.csv"),
      col_types = "cccccc"
    )
  ) %>%
    pluck("lower")

  write_csv(claassen_input_raw,
    file = here::here(
      "data",
      "claassen_input_raw.csv"
    )
  )
}

claassen_input_raw <- read_csv(
  here::here(
    "data",
    "claassen_input_raw.csv"
  ),
  col_types = "cdcddcd"
)

claassen_input_raw1 <- claassen_input_raw %>%
  filter(!((
    str_detect(item, "army_wvs") &
      # WVS obs identified as problematic by Claassen
      ((country == "Albania" & year == 1998) |
        (country == "Indonesia" &
          (year == 2001 | year == 2006)) |
        (country == "Iran" & year == 2000) |
        (country == "Pakistan" &
          (year == 1997 | year == 2001)) | # 1996 in Claassen
        (country == "Vietnam" & year == 2001)
      )
  ) |
    (
      str_detect(item, "strong_wvs") &
        ((country == "Egypt" & year == 2012) |
          (country == "Iran" &
            (year == 2000 | year == 2007)) | # 2005 in Claassen
          (country == "India") |
          (country == "Pakistan" &
            (year == 1997 | year == 2001)) | # 1996 in Claassen
          (country == "Kyrgyzstan" &
            (year == 2003 | year == 2011)) |
          (country == "Romania" &
            (year == 1998 | year == 2005 | year == 2012)) |
          (country == "Vietnam" & year == 2001)
        )
    ) |
    (
      country %in% c(
        "Puerto Rico",
        "Northern Ireland",
        "SrpSka Republic",
        "Hong Kong SAR China"
      )
    ))) %>%
  with_min_yrs(2)

claassen_input0 <- DCPOtools::format_claassen(claassen_input_raw1)

cri <- claassen_input0$data %>%
  mutate(
    p_dcpo = str_extract(survey, "^[a-z]+"),
    project = case_when(
      p_dcpo == "afrob" ~ "afb",
      p_dcpo == "amb" ~ "lapop",
      p_dcpo == "arabb" ~ "arb",
      p_dcpo == "asiab" ~ "asb",
      p_dcpo == "asianb" ~ "asnb",
      p_dcpo == "neb" ~ "ndb",
      p_dcpo == "sasianb" ~ "sab",
      TRUE ~ p_dcpo
    ),
    item_fam = str_extract(item, "^[a-z]+"),
    item_fam = if_else(item_fam == "election", "elec", item_fam),
    dataset = "cri"
  )

supdem_cri <- full_join(sd, cri, by = c("country", "year", "item_fam", "project"))

# While DCPO assigns actual year of survey, Claassen (2020) often uses nominal year
# of wave, so some corrections are required to match observations (~8% of obs);
# we could write about this as a data-entry error, too, but no space to spare,
# so we'll adopt those years instead to facilitate straight-up comparison
no_problems <- inner_join(sd %>% select(-dataset),
  cri %>% select(-dataset),
  by = c("country", "year", "item_fam", "project")
) %>%
  mutate(diff_year = 0) # 3403 obs

needed <- anti_join(
  sd %>% select(-dataset),
  cri %>% select(-dataset)
) # 313 obs

available <- anti_join(
  cri %>% select(-dataset),
  sd %>% select(-dataset)
) # 1310 obs

year_fixes <- left_join(needed, # 304 obs
  available,
  by = c("country", "project", "item_fam")
) %>%
  mutate(diff = year.x - year.y) %>%
  group_by(country, project, item_fam, year.x) %>%
  mutate(closest_to_claassen = min(abs(diff))) %>%
  ungroup() %>%
  group_by(country, project, item_fam, year.y) %>%
  mutate(closest_to_dcpo = min(abs(diff))) %>%
  ungroup() %>%
  filter(closest_to_claassen == abs(diff) & closest_to_dcpo == abs(diff) & abs(diff) <= 3) %>%
  filter(!(country == "Egypt" & year.x == 2014 & survey == "afrob5")) %>% # double matches (it's really afrob6)
  mutate(
    year = year.y,
    diff_year = abs(diff)
  )

cys_crosswalk <- year_fixes %>%
  select(country, y_dcpo = year.y, y_claassen = year.x, survey) %>%
  distinct()

claassen_input_raw2 <- claassen_input_raw1 %>%
  # left_join(cys_crosswalk, by = c("country", "year" = "y_dcpo", "survey")) %>%
  # mutate(year = if_else(is.na(y_claassen), year, y_claassen),
  #        p_dcpo = str_extract(survey, "^[a-z]+"),
  #        project = case_when(p_dcpo == "afrob" ~ "afb",
  #                            p_dcpo == "amb" ~ "lapop",
  #                            p_dcpo == "arabb" ~ "arb",
  #                            p_dcpo == "asiab" ~ "asb",
  #                            p_dcpo == "asianb" ~ "asnb",
  #                            p_dcpo == "neb" ~ "ndb",
  #                            p_dcpo == "sasianb" ~ "sab",
  #                            TRUE ~ p_dcpo)) %>%
  # right_join(sd %>%
  #             distinct(country, year, project),
  #           by = c("country", "year", "project")) %>%
  right_join(sd %>% distinct(country), by = "country")

claassen_input <- DCPOtools:::format_claassen(claassen_input_raw2)

cri2 <- claassen_input$data %>%
  mutate(
    p_dcpo = str_extract(survey, "^[a-z]+"),
    project = case_when(
      p_dcpo == "afrob" ~ "afb",
      p_dcpo == "amb" ~ "lapop",
      p_dcpo == "arabb" ~ "arb",
      p_dcpo == "asiab" ~ "asb",
      p_dcpo == "asianb" ~ "asnb",
      p_dcpo == "neb" ~ "ndb",
      p_dcpo == "sasianb" ~ "sab",
      TRUE ~ p_dcpo
    ),
    item_fam = str_extract(item, "^[a-z]+"),
    item_fam = if_else(item_fam == "election", "elec", item_fam),
    dataset = "cri2"
  )

supdem_cri2 <- full_join(sd,
  cri2,
  by = c("country", "year", "item_fam", "project")
)

no_problems2 <- inner_join(sd %>% select(-dataset),
  cri2 %>% select(-dataset),
  by = c("country", "year", "item_fam", "project")
) %>%
  mutate(year_correct = TRUE) # 3706 obs

needed2 <- anti_join(
  sd %>% select(-dataset),
  cri2 %>% select(-dataset)
) # 10 obs; these are not in the surveys

data_comparison <- bind_rows(no_problems, year_fixes) %>%
  mutate(
    perc_cls = Response / Sample * 100,
    perc_ours = x / samp * 100,
    diff_perc = perc_cls - perc_ours,
    diff_perc_abs = abs(diff_perc),
    diff_x = round(Response - x),
    diff_samp = round(Sample - samp)
  ) %>%
  select(-CAbb, -COWCode)

save(data_comparison, file = here::here("data", "data_comparison.rda"))
```

```{r comparison-dcpo}
#| label: fig-comparison
#| fig-cap: "Comparing Democracy-Supporting Responses in Hand-Entered and Machine-Collected Data."
#| fig-align: "center"
#| fig-height: 5

load(here::here("data", "data_comparison.rda"))

bad_items <- c(
  "Asia Barometer:\nEvaluation of Democracy",
  "Asian Barometer II:\nSuitability of Democracy",
  "Pew Global Attitudes:\nImportance of Democracy",
  "Other Items"
)

plot_support <- data_comparison %>%
  mutate(p_dcpo3 = case_when(
    Item == "EvDemoc_asb" ~ "Asia Barometer:\nEvaluation of Democracy",
    Item == "DemocSuit_asnb" & dplyr::between(year, 2005, 2008) ~ "Asian Barometer II:\nSuitability of Democracy",
    Item == "ImpDemoc_pew" ~ "Pew Global Attitudes:\nImportance of Democracy",
    TRUE ~ "Other Items"
  ) %>%
    factor(levels = bad_items)) %>%
  ggplot(aes(x = perc_ours, y = perc_cls)) +
  geom_point(aes(color = p_dcpo3, shape = p_dcpo3), size = 1.5) +
  labs(
    y = "Hand-Entered Data",
    x = "Machine-Collected Data"
  ) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  theme_bw() +
  theme(
    legend.justification = c(1, 0),
    legend.position = c(.99, .01),
    legend.text = element_text(size = 6),
    legend.background = element_rect(fill = "transparent"),
    legend.key.size = unit(1.1, "lines")
  ) +
  scale_colour_manual(
    name = element_blank(),
    values = alpha(
      c(
        "gray50",
        "gray10",
        "gray35",
        "gray50"
      ),
      c(1, 1, 1, .3)
    ),
    breaks = bad_items
  ) +
  scale_shape_discrete(
    name = element_blank(),
    breaks = bad_items
  )

tx_note <- str_wrap("Notes: Each point represents the percentage of respondents in a country-year to give a democracy-supporting response to a particular survey item. Hand-entered data is as reported in Claassen (2020c); the machine-collected data was collected directly from the original surveys. Only 85% of the observations, the difference between these percentages was negligible—less than half a percent—yielding points approximately along the plot’s dotted line. The Asia Barometer's item on the evaluation of democracy accounts for most overreports, and the Pew Global Attitudes item on the importance of democracy accounts for most substantial underreports.  In both cases, as well as the overreports of the suitability of democracy item in the second wave of the Asian Barometer, the issues can be easily explained by errors in transcribing the data in accordance with the reported coding rule.  Deviations in other items result from inconsistent treatment of missing data and/or survey weights, reflecting in part differences in codebook reporting practices across surveys.",
  width = 96
)

plot_support +
  patchwork::plot_annotation(caption = tx_note) &
  theme(plot.caption = element_text(hjust = 0))

```

For about `r {mean(data_comparison$diff_perc_abs < .5)*100} %>% round()`% of country-year-item observations, differences between percentages are negligible-less than half a percent-resulting in points near the plot’s diagonal. 
However, for the remaining cases, differences are significant and cannot be ignored.
These discrepancies stem from decisions on categorization, processing numerators and denominators in ratio calculation, and whether counting respondents excluded from original surveys (see more details in @sec-comparison). 
Though errors introduced during such processes seems minor and may appear as random noise, they can lead to opposite conclusions [@HuEtAl2024a]. 
Such issues pose a major, yet underrecognized, challenge to data harmonization in political science.

In this article, we provide a practical routine taken the advantage of automatic programming and team work to reduce such data-entry errors and improve the reproducibility and transparency of the wrangling process for researchers and reviewers to check the errors.
The routine includes three steps: data selection, data entry, and opening.
We illustrate how researchers use this routine on statistical (*hard*) and opinion (*soft*) data with two ongoing harmonization efforts, the Standardized World Income Inequality Database (SWIID) and the Dynamic Comparative Public Opinion (DCPO) project.


# A 3-Step Routine for Data Harmonization

Our routine aims to helping researchers reach three goals for scientific research: 

1. To incorporate as much available data as possible to provide base for comparable data and increase generality of the inferences;
1. To reduce the manual entry errors to improve the accuracy of the harmonized data and analytic data; and 
1. To improve the reproducibility of data wrangling process for the sake of transparency.

The routine decomposes a data-wrangling process into three steps: 

1. Team-based concept construct and data selection;
1. Data entry automation; and 
1. "Second-order" opening.

To illustrate the above routine, we use two data harmonization projects as examples, SWIID and DCPO.
SWIID is a long-running project that seeks to provide harmonized income inequality statistics for the broadest possible coverage of countries and years [@Solt2009; @Solt2015; @Solt2016; @Solt2020].
As of its most recent update at the time of this writing, its source data consists of some 27,000 observations of the Gini coefficient of income distribution in nearly 200 countries over as many as 65 years, collected from over 400 separate sources including international organizations, national statistics bureaus, and academic studies.

DCPO is both a method and a database.
Scholarship on comparative public opinion only rarely benefits from relevant items asked annually by the same survey in many countries [see, e.g., @HagemannEtAl2017].
To address the lack of cross-national and longitudinal data on many topics, a number of works have presented latent variable models that harmonize available but incomparable survey items [see e.g., @CaugheyEtAl2019; @Claassen2019; @KolczynskaEtAl2024].
Along this line, DCPO not only provides latent variable measurements but also automatized and reproducible data collection [@Solt2020], which has been applied in a complete pipeline for a variety of topics such as gender egalitarianism [@WooEtAl2023], political interest [@HuSolt2024a], and support for gay rights [@WooEtAl2024], among other aspects of public opinion and open it freely for global researchers (see more updated data collections at <https://dcpo.org/>).

In the following sections we first address the common challenges for the phases of data wrangling and explain how our routine can help deal with it illustrated with the data wrangling processes of the SWIID and DCPO projects.

## Step 1: Team-Based Construct Building and Data Selection

Large scale of data selection and cleaning is almost always tedious, as something to be delegated to research assistants, to someone---indeed anyone, but usually research assistants (RA)---else [see @Torres2017].
This manual procedure is easy to make mistakes and errors.
@HaegemansEtAl2019 [p. 1] has demonstrated examples of misrouted financial transactions and airline flights.
In a more systematic examination, @BarchardPace2011 found that RA assigned in an experiment to carefully enter data manually, even those instructed to double-check their entries against the original, had error rates approaching 1% in just a single roughly half-hour session.
The consequences of such errors can be pernicious.

Our antidote for this issue is a combination of team work and automation. 
We will focus more on the team work and discuss the latter in @sec-automate.
The goal here is to have consistent understanding on conceptualized construct, select valid data for later measurement and/or analyses, and reduce biases caused by inconsistent human judgment.
A team work framework for this end requires a deliberative set and a dual-entry process.

A deliberative set requires the members in a research team---regardless several coauthors or a primary author with one or two RAs---to have a clear and coherent understanding of the reseach questions and associated data goals.
These understandings will help the team members identify the right data to collect and discover extra useful data sources that are not in the initial plan.

In the SWIID program, for example, we told RAs that the goal of the research is to generate comparable statistics of country-level economic inequality.
We provide a list of sources mainly from national statistic bureaus for them to start, but we also told them that update statistics for some countries may come from academic papers, published documents, and other sources, and they are free to add them in while making sure a valid link of the new sources are also recorded.

Ensuring team members to understand how the data would use later is also important, as they could have a better sense of what data are analyticable and a forward perspective of how many situations would the later entry part need to take care. 
In the SWIID project, we told the RAs that the inequality statistics be recorded in four formats: Gini index in disposable (post-tax, post-transfer) income, Gini in market (pre-tax, pre-transfer) income, absolute redistribution (market-income inequality minus net-income inequality), or relative redistribution (market-income inequality minus net-income inequality, divided by market-income inequality).
So, for later unification work, they need not only to record the digits but also seek documents to explain the methods of the statistics.

The SWIID project requires update for almost every year and we also often hire new RAs.
Therefore, the cross-check is done in a rolling basis usually by the rookies who are in charge of checking the old data and updating malfunctional links.
This is both a learning process and a way to improve data accuracy.


In the DCPO project, clearly defining and agreeing upon the latent construct among team members is a critical first step for ensuring theoretical comparability across countries and over time  [@Koc2025]. 
This process begins with a shared conceptual foundation established through literature review and corresponding pre-defined potential dimensions of the latent opinion.
Each team member is then assigned survey datasets from specific geographic regions and tasked with identifying potentially relevant items and potential dimensions based on both general theoretical guidance and region-specific knowledge. 
This structure ensures that the construct is informed by both global theory and local context.

Before data selection begins, team members undergo hands-on training on how the method work and what type of data and detail they need to collect, such as data format and weighting types, which provide a valuable help of later build the automative data preparation software. 

Following the initial round of item selection and collection, the dural-entry section comes in. 
In this stage, each team member reviews and re-codes the survey data originally handled by another member.
The independently coded versions are then compared to detect discrepancies, which may arise from misinterpretations of the construct, ambiguous item wording, or common entry errors.

Disputed cases are flagged for group discussion.
Some mismatches may indicate items that may not be conceptually equivalent across cultures or regions, and others suggest multidimensionality that requires theoretical disaggregation.
For the latter, we either categorize such items into pre-defined dimensions and/or revise the codebook accordingly to add new dimensions—an iterative process aimed at improving construct validity, intercoder reliability, and reducing oversimplification of target variable [@Slomczynsi2025].

Therefore, we broke down the cross-check step into several lab meetings interspersed during the data selection to collect new insights from each members' selection works and make sure everyone were on the same page through the whole process.
The process ends with a systemic cross-check of the final selected data among members. 

In the DCPO project, the process begins with a shared conceptual foundation through literature review and pre-defined dimensions of latent opinion.
Clearly defining the latent construct among team members is essential for ensuring theoretical comparability across countries and time [@Koc2025]
Then, team members are assigned datasets from specific regions to identify relevant items based on both theoretical guidance and regional knowledge. 
After initial item selection, each member reviews another's coded data to identify discrepancies from misinterpretations or ambiguous items, a.k.a., a dual-entry process.

Disputed cases are flagged for later group discussions, revealing items lacking cross-cultural equivalence or suggesting multidimensionality requiring theoretical disaggregation.
The iterative process aimed at improving construct validity and reducing oversimplification [@Slomczynsi2025]. 
The team conducts several lab meetings during data selection to incorporate insights and ensure alignment throughout the process.

## Step 2: Data Entry Automation {#sec-automate}

Formatting data is arguably the easiest step to involve manual errors and controversies.
The best solution is to automate the entry process taken the advantages of the programming languages and application programming interfaces (APIs) of the data source.

In the DCPO case, data entry is fully automated through the R-based software, `DCPOtools` [@SoltEtAl2018a]. 
This software processes raw survey files directly, ensuring reproducible data entry. 
It converts various file formats to R-readable objects, extracts variables of interest, reorders response values, applies survey weights, and aggregates weighted respondents by country and year based on actual fieldwork dates.

To address theoretical comparability concerns, DCPO employs conservative filtering, removing items appearing in fewer than five country-years in countries surveyed at least three times, minimizing the risk of sacrificing comparability for coverage [@Koc2025].
`DCPOtools` standardizes country names using @Arel-BundockEtAl2018's `countrycode` and ensures years reflect actual fieldwork dates, creating aggregated respondent data for the latent variable model.


```{r}
#| label: data
#| include: false

api <- c("LISSY", "CEPALStat", "OECD", "Eurostat", "Beegle et al. 2016", "Statistics Canada", "Statistics Denmark", "Statistics Finland", "CSO Ireland", "Statistics Norway", "Statistics Sweden", "World Bank Poverty & Inequality Platform", "Statbank Greenland")
sheet <- c("SEDLAC", "Transmonee 2012", "Personal communication, K. Beegle, 2016-08-01", "World Bank Povcalnet", "Australian Bureau of Statistics", "Instituto Naciónal de Estadística de Bolivia", "Instituto de Pesquisa Económica Aplicada", "Departamento Administrativo Nacional de Estadística Colombia", "Instituto Naciónal de Estadística y Censos Costa Rica", "Central Agency for Public Mobilization and Statistics Egypt", "Statistics Estonia", "Statistics Georgia", "Statistics Hong Kong 2017", "Statistics Indonesia", "Istat", "Statistical Institute of Jamaica", "Kazakhstan Committee on Statistics", "Statistics Korea", "National Statistical Committee of Kyrgyzstan", "National Bureau of Statistics of Moldova", "Statistical Office of Montenegro", "Statistics New Zealand 1999", "Philippines Statistical Agency", "Russian Federal State Statistics Service", "Singapore Department of Statistics", "Slovenia Statistics Office", "Slovenia Statistics Office 2005", "Instituto Nacional de Estadística Spain", "Switzerland Federal Statistics Office", "Taiwan Directorate General of Budget, Accounting, and Statistics", "Turkish Statistical Institute", "UK Office for National Statistics", "Institute for Fiscal Studies", "U.S. Congressional Budget Office", "U.S. Census Bureau", "Instituto Nacional de Estadística Venezuela", "Milanovic 2016", "Milanovic 2016; Brandolini 1998", "Ackah, Bussolo, De Hoyos, and Medvedev 2008", "NESDC Thailand", "U.S. Census Bureau 1998", "Dirección General de Estadística, Encuestas y Censos")
scrape <- c(
  "National Statistical Service of Armenia", "Belarus National Committee of Statistics", "Statistics Hong Kong 2012", "Statistics Hong Kong 2007", "Dirección General de Estadística, Encuestas y Censos 2016", "Economy Planning Unit of Malaysia", "Perry 2018", "Dirección General de Estadística, Encuestas y Censos 2017", "Statistics Sri Lanka 2015", "NESDB Thailand", "Instituto Nacional de Estadística Uruguay", "General Statistics Office of Vietnam 2013", "General Statistics Office of Vietnam", # <- scrape from pdf
  # webscrape ->
  "Institut National de la Statistique et des Études Économiques France", "Statistical Center of Iran", "National Statistical Office of Thailand"
)

length(api) <- length(sheet)
length(scrape) <- length(sheet)

mode <- tibble(api, sheet, scrape) %>%
  gather(key = mode, value = source1) %>%
  filter(!is.na(source1))

swiid_source <- read_csv("https://raw.githubusercontent.com/fsolt/swiid/master/data/swiid_source.csv",
  col_types = "cdddcclcccc"
) %>%
  left_join(mode, by = "source1") %>%
  mutate(mode = if_else(is.na(mode), "hand", mode))

wordify_numeral <- function(x) setNames(c("one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen", "sixteen", " seventeen", "eighteen", "nineteen"), 1:19)[x]

api_percent <- swiid_source %>%
  count(mode == "api") %>%
  mutate(p = round(n / nrow(swiid_source) * 100)) %>%
  filter(`mode == "api"` == TRUE) %>%
  pull(p)

automated_percent <- swiid_source %>%
  count(mode == "hand") %>%
  mutate(p = round(n / nrow(swiid_source) * 100)) %>%
  filter(`mode == "hand"` == FALSE) %>%
  pull(p)

```

```{r}
#| label: fig-dataMethod
#| echo: false
#| fig-cap: "Income Inequality Observations by Method of Collection"
#| fig-height: 2
#| fig-width: 4


swiid_source %>%
  count(mode) %>%
  mutate(method = fct_recode(factor(mode),
    "API" = "api",
    "Spreadsheet\nDownload" = "sheet",
    "Web/PDF\nScrape" = "scrape",
    "Hand\nEntered" = "hand"
  ) %>%
    fct_relevel(
      "Hand\nEntered",
      "Web/PDF\nScrape",
      "Spreadsheet\nDownload",
      "API"
    )) %>%
  ggplot(aes(x = n, y = method, fill = method)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  theme(axis.title.y = element_blank()) +
  xlab("Observations") +
  scale_fill_manual("Legend", values = c(
    "Hand\nEntered" = "gray75",
    "Web/PDF\nScrape" = "gray10",
    "Spreadsheet\nDownload" = "gray10",
    "API" = "gray10"
  )) +
  theme(legend.position = "none")

# Hand-Entered and Machine-Collected
```

While coding datasets and items into structured spreadsheets facilitates automation, an even better version starts the automation since the data selection step via programming and APIs.
As shown in @fig-dataMethod, the current version of SWIID grapes `r api_percent`% of the observations through API.
When no API is available, the automation script downloads and reads any available spreadsheets [see @Wickham2016].
In the absence of a spreadsheet, the process of scraping the data either directly from the web or, preferably, from a pdf file [see @Sepulveda2024] is automated.
Together the collection of `r automated_percent`% of the source data is scripted.
This means not only that the possibility of errors introduced by hand entry for a vast majority of observations is eliminated but also that the updates and revisions that are frequent in these data are automatically incorporated as they become available.^[
  The R community has often built software to ease the access of APIs and make the batch work for multiple waves of data in a more comfortable and efficient way [see @Blondel2018; @MagnussonEtAl2014; @LahtiEtAl2017; @Lugo2017; @WickhamEtAl2018].
]

For data sources, such as those from academic articles or books, that have to be entered in hand, there is still rooms for automation. 
For the remaining `r 100 - automated_percent`% of the SWIID observations, for instance, we collected them using Sepulveda's `tabulapdf` R package to avoid data-entry errors as long as they are in pdf [@Sepulveda2024].
The advanced Optical Character Recognition (OCR) can extend this method on data sources even in hard copies.

And finally, one source of SWIID contains crucial information encoded in the typeface of its tables [see @MitraYemtsiv2006, p. 6]; this information would be lost if the tables were read directly into R.
We reapplied the approach from the data selection here to enter them twice into separate spreadsheets.^[Most often this has been done by two different investigators, but sometimes sequentially by a single researcher.]
The dual-entry process allows for automated cross-checks of the newly entered data that increase the chances that errors are identified and corrected [see @BarchardPace2011].


## Step 3: "Second-Order" Opening

Since the replication crisis, replication files for analytical results in academic articles has become a standard requirement for top-tier journals in political science [@ChangLi2015; @OpenScienceCollaboration2015]. 
Nevertheless, the continual raising controversies on the researcher degrees of freedom indicated that current open is still not adequate.^[See a summary of the "researcher degrees of freedom" literature in @HuEtAl2024a.]
Especially in relation with data harmonization, we eager researchers to conduct a, what we called, the "second-order" opening. 
That is, not only opening analytical steps (the "first-order") but also the data generation process (the "second-order"), including data collection, data cleaning, and data wrangling, as mentioned above.

If researchers applied our suggestions of team-based construct building, systematic data selection, and automated data entry, the second-order opening becomes both feasible and efficient.
Along with a clearly conceptualized theoretical framework, researchers can simply share their programming scripts for data downloading, formatting, and wrangling, ensuring that the full pipeline is documented and reproducible.

With developed scientific and technical publishing system, such as Quarto or R markdown, and version control platforms (e.g., Github) and open collaboration platforms (e.g., Open Science Framework, OSF), researchers can integrate the entire workflow—from raw data collection to final analysis—within a single, publicly trackable archive.
We reached at this step for all the DCPO projects so far. 
Readers can find a Github repo for the research from scratch, and every wave of data update in the corresponding OSF project.^[See a comprehensive example applied the second-order opening strategy in @TaiEtAl2024. ]


<!--Readers can find a comprehensve example applied the above second-order openning strategy in a DCPO project, @TaiEtAl2024. 
Its replication data includes three types of files beside a instruction (.md): a Rmarkdown file recording all the codes we wrangled and analyzed the data together with the main text and appendices, several auxiliary files (.lua, .bib) for the Rmarkdown to render, and data files for running each chunk of codes in the Rmarkdown [@TaiEtAl2022].
Moreover, readers can track the entire process of how the project grew through an publicly open Github repo, <https://github.com/fsolt/dcpo_dem_mood>.
They can also see the data updating history and outcomes in an OSF project, <https://osf.io/tnp2a/>.
Similarly, people can see the updating history of the in  SWIID dataset in <https://github.com/fsolt/swiid>.-->

# Discussion

![](images/data_diagram.png){#fig-diagram fig-align="center" height=400}

Implementing these open-science practices requires effort [see @EngzellRohrer2021]. 
Though labor-intensive, the double-entry method reduces error rates thirty-fold [@BarchardPace2011, p. 1837], justifying the investment. 
Teamwork distributes tasks, reducing fatigue-related errors, while allowing discrepancies to be resolved through discussion.

Social scientists now benefit from standardized harmonization workflows [@Slomczynsi2025] and automated data processing [@Kritzinger2025]. 
Researchers can reuse high-quality harmonized datasets, enhancing efficiency and comparability.
Open-source software packages like those used by the SWIID and `DCPOtools` have already automated many data preparation tasks. 
With large language models emerging, intelligent agents may soon handle parts of these routines, potentially advancing automation to new levels [@Kritzinger2025].

A final point we would like to clarify is that, in our three-step routine, researchers remain central to harmonization. 
As illustrated in the SWIID and DCPO examples, researchers are responsible for all critical decisions from clarifying research questions and building theoretical constructs to conducting version control and developing replication materials.
Early and critical steps, such as construct development and codebook refinement, must be conducted iteratively to achieve high intercoder reliability. 
Even with automated data entry, human validation remains essential for verifying variable formats and value ranges. Computing environments should be documented to minimize system-related discrepancies [@Liu2019].

For ex-post harmonization projects, careful attention to pre-harmonization stages substantially contributes to overall dataset quality. 
While some error is inevitable, with responsible researcher oversight, data-entry errors can be minimized while transparency, openness, and research credibility continue to grow.


\newpage
\theendnotes

# References {.unnumbered}

::: {#refs-main}
:::

\pagebreak

\hypertarget{appendix-appendix}{%
\appendix}

# Online Supplementary Materials {.unnumbered}

\setcounter{page}{1}
\renewcommand{\thepage}{A\arabic{page}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}

# Sources of differences shown in @fig-comparison {#sec-comparison}

Comparing the original hand-entered dataset [@Claassen2020d] with data that were machine-collected using the `DCPOtools` package for R [@Solt2018] revealed three survey items for which the hand-entered data did not match the data's documented coding rules.
These rules indicate that responses above the median value in the response scale are to be considered as supporting democracy, while those at the median value and below are not [see @Claassen2020b, Appendix 1.3].

First, the Asia Barometer asked respondents in 35 country-years to indicate whether they thought "a democratic political system" would be very good, fairly good, or bad for their country.
According to the original study's coding rules [see @Claassen2020b, Appendix 1.3], only answers above the median of the response categories should be considered as democracy supporting, yet in this case the lukewarm intermediate category was coded as supporting democracy as well.
Similarly tepid responses at and below the median response category to similar questions (e.g., in the Arab Barometer, that democracy was "somewhat appropriate" for the country) were coded as not supportive, confirming that this is indeed a data-entry error.
This discrepancy resulted in hand-entered percentages of democracy-supporting responses ranging from `r data_comparison %>% filter(item=="evdemoc_asiab") %>% pull(diff_perc) %>% min() %>% round()` to `r data_comparison %>% filter(item=="evdemoc_asiab") %>% pull(diff_perc) %>% max() %>% round()` percentage points higher than the data automatically collected directly from the survey datasets.
<!-- and averaging `r data_comparison %>% filter(item=="evdemoc_asiab") %>% pull(diff_perc) %>% mean() %>% round()` points higher. -->

Second, the four waves of the Asian Barometer included the following item: "Here is a similar scale of 1 to 10 measuring the extent to which people think democracy is suitable for our country. If 1 means that democracy is completely unsuitable for [name of country] today and 10 means that it is completely suitable, where would you place our country today?"
In accordance with the coding rules of the study, responses of 6 through 10 are considered democracy supporting, and that is how the first, third, and fourth waves of the survey are coded.
For the second wave, however, 5 was erroneously also included among the democracy-supporting responses.
This data-entry error resulted percentages overstated by as much as `r data_comparison %>% filter(item=="democsuit_asianb" & (year==2005|year==2006|year==2008)) %>% pull(diff_perc) %>% max() %>% round()` percentage points in 9 country-years. 

And third, the Pew Global Attitudes surveys' four-point item asking about the importance of living in a country with regular and fair contested elections: the question wording is "How important is it to you to live in a country where honest elections are held regularly with a choice of at least two political parties? Is it very important, somewhat important, not too important or not important at all?"
In this case, rather than including respondents who gave both responses above the median---"very important" and "somewhat important"---only those respondents who answered "very important" were entered as supporting democracy.
This error caused the hand-entered percentages to be substantially lower in 91 country-years.